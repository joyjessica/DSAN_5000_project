[
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "LLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nChat GPT was used to brainstorm project ideas and provide feedback and refine the project plan.\nUse Chat GPT to decide what relevant columns to get from 10,000+ columns of Census Data"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nProofreading\nText summarization for literature review\nUsed to convert formulas written into Latex format for readibility\nThink of a fun engaging title"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation\nChatGPT was used to aid in generating code to navigate the American Community Survey Census API"
  },
  {
    "objectID": "index.html#motivation",
    "href": "index.html#motivation",
    "title": "Breaking the Cycle: How School and Socioeconomics Shape High School Dropout and College Success",
    "section": "Motivation:",
    "text": "Motivation:\nDropping out of high school has often been linked to many long term negative outcomes. Research shows that students who drop out of high school are more likely to face unemployment, earn lower wages, engage in high-risk behaviors, become incarcerated, and impose significant costs on society1. Through this website, I aim to explore the factors that potentially contribute to high school dropout rates, with the goal of identifying opportunities for early intervention to mitigate these negative outcomes. Particularly, I wanted to look at how these metrics manifest in New York City public schools. My focus will be on understanding how school quality and socioeconomic factors influence student outcomes, specifically chronic absenteeism (missing more than 10% of the school year), dropout rates, and college persistence (returning to college after the first year).\nThis research is intended to inform policymakers, researchers, and data enthusiasts interested in understanding the factors that contribute to chronic absenteeism, dropout rates, and college persistence. I hypothesize that these three outcomes are interconnected. For instance, high chronic absenteeism may correlate with higher dropout rates, and even if students avoid dropping out of high school, could there still be a risk of low college persistence? These are the key questions I aim to address through this research.\n\nData Science Questions:\n\nHow does a school’s environment contribute to better student outcomes?\nWhat are the most significant predictors of student absenteeism, dropout rates and college persistence?\nWhat are the interactions between chronic absenteeism, dropout rates, and college persistence?\nHow does socioeconomic status impact student absenteeism, dropout rates and college persistence?\nHow can early academic indicators be used to identify schools at risk of high dropout or not persisting in college?\n\n\n\nRelevant Current Research\nSchool building condition, school attendance, and academic achievement in New York City public schools: A mediation model: This study, published in the Journal of Environmental Psychology in 2008, examined the relationship between school building condition and the academic achievement of students in elementary schools in Manhattan, New York City. The researchers found that school building condition was positively related to student achievement in mathematics and English Language Arts. They also found that attendance important to this relationship, suggesting that students in better-quality buildings are more likely to attend school, which in turn leads to better academic outcomes.2\nA Public Health Perspective on School Dropout and Adult Outcomes: A Prospective Study of Risk and Protective Factors from Age 5 to 27: This research paper investigates the long-term consequences of high school dropout, framing it as a significant public health issue. A study followed 585 individuals from age 5 to 27, analyzing the correlation between dropping out and negative outcomes like unemployment, arrest, and poor health, finding a strong link between them. The researchers identified several risk and protective factors, such as low socioeconomic status at age 5 and peer rejection in elementary school. The findings highlight the importance of early intervention and suggest that addressing school dropout proactively could significantly improve individual lives and reduce societal costs.1\nNeighborhood Poverty and Public Policy: A 5-Year Follow-Up of Children’s Educational Outcomes in the New York City Moving to Opportunity Demonstration: This research paper analyzes the long-term effects of the Moving to Opportunity (MTO) program, an experiment that provided housing vouchers to low-income families in high-poverty neighborhoods to relocate to lower-poverty areas. The study focuses on a subset of New York City children followed for five years, comparing their educational outcomes with those who remained in high-poverty neighborhoods. The results revealed negative effects on grades and school engagement for both boys and girls in low-poverty neighborhoods. The study highlights the necessity of improving educational outcomes for low-income minority children, focusing on family, neighborhood, housing, and school factors.3\n\n\nNext Steps\nThis website offers data driven insights, visualizations, and machine learning models to dive into this topic. I invite you to explore the content!"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\n\n\n\n\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges-1",
    "href": "technical-details/data-collection/closing.html#challenges-1",
    "title": "Summary",
    "section": "Challenges",
    "text": "Challenges\nOne major challenge faced was finding the data I needed that could be aggregaated to my dataset. The NYC high school quality data is aggregated by school name and DBN, which is a uniue new york identifier that uses the district, bourough, and nyc doe school number. However many other datasets, for example country wide ones, that I was interested in getting features from did not include these aggregations since it appears to be new york specific. Therefore I had to spend a lot of time and was a little limited to finding data with this aggregation. I was able to find a mapping of DBN to zip code, which allowed me to map the census data to my school data, incorporating a few socioeconomic factors.\nIn future work, I would be interesting is scaling this project to all of New York, or even country wide. Since I am only looking at New York City high schools, I only have about 500 rows, which is a bit of a small dataset. If this can be scaled to a larger amount of schools, I would probably be able to create a more robust dataset and models."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps-1",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps-1",
    "title": "Summary",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nHowever, we by aggreagating data from 3 different datasets, we are able to evaluate many features that may have impacts on our target varaibles and help us evaluate how high school qualities and socioeconomic factors effect high school outcomes in New York City."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis.\n\nIn this section, the goal is to build machine learning models to investigate the drivers of our target variables: chronic absenteeism, dropout risk, and college persistence. We will create three different models: a regression, a binary classification, and a multivariate classification. For each, we will test various algorithms and identify the one that performs best.\nI developed several functions to streamline the modeling process. These functions handle preprocessing, training, tuning, scoring the validation set, and scoring the test set.\n\nPreprocessing: This function takes a dataframe, target column, and test size as inputs. It splits the data into training, validation, and test sets. The training set is used to train the models, the validation set is crucial for tuning hyperparameters, and the test set evaluates the model’s performance on unseen data.\nTrain: This function accepts X_train, y_train, learning_task_type, model_type, and kwargs. The learning_task_type should be either ‘classification’ or ‘regression’, while model_type specifies the algorithm to be used. The kwargs parameter allows additional arguments to be passed to the algorithm, which is used once the model is tuned.\nTune: This function takes a model, X_train, y_train, param_grid, and an optional scoring parameter. It uses cross-validation to tune hyperparameters. Predefined parameter grids for random forest and gradient boosting models allow the function to perform a grid search over specified values to identify optimal parameters. The scoring parameter specifies the metric to maximize, depending on the task.\nValidation Scoring: This function evaluates the model’s performance on the validation set to guide hyperparameter tuning. It accepts learning_task_type, model, X_val, and y_val as inputs. For regression tasks, it outputs metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R². For classification tasks, it provides a confusion matrix.\nEvaluate Test: This is the final step, where the model’s performance is evaluated on the test set. The function accepts learning_task_type, model, X_test, and y_test as inputs. It outputs relevant regression or classification metrics for the tuned mode\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn import datasets, linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import Lasso, LassoCV\n\nimport statsmodels.api as sm\nfrom scipy import stats\n\n\ndf = pd.read_csv('../../data/processed-data/processed_df.csv', index_col=None)\ndf_regression = pd.read_csv('../../data/processed-data/df_subset_regression.csv', index_col=None)\n\n\n# Drop categorical and identifier columns we dont need\ndf.drop(columns=['DBN', 'School Name', 'District', 'zip code', 'Borough'], inplace=True)\n#drop columns that cause perfect multicollinearity\ndf_clean = df.drop(['Median Household Income',\"Percent Bachelor's Degree or Higher (25+)\",'Percent No High School (25+)','Student Percent - Other', 'Percent Female','Percent College Ready based on SAT Math'], axis=1)\n\n\ndef preprocessing(df, target_column, test_size=0.2):\n       \"\"\"Splits the data into training and test sets.\"\"\"\n       X = df.drop(columns=[target_column])\n       y = df[target_column]\n       #split into training, validation, test set\n       X_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=test_size, shuffle=True)\n       X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=test_size, shuffle=True)\n       #standardize\n       scaler = StandardScaler()\n       X_train = scaler.fit_transform(X_train)\n       X_val = scaler.transform(X_val)\n       X_test = scaler.transform(X_test)\n       return X, y, X_train, X_val, X_test, y_train, y_val, y_test\n\n\ndef train(X_train, y_train, learning_task_type, model_type, **kwargs):\n    \"\"\"Trains a machine learning model.\"\"\"\n    if learning_task_type == 'regression':\n        if model_type == 'random_forest':\n            model = RandomForestRegressor(**kwargs)\n        elif model_type == 'linear_regression':\n            model = LinearRegression(**kwargs)\n        elif model_type == 'gradient_boosting':\n            model = GradientBoostingRegressor(**kwargs)\n        else:\n            raise ValueError(\"Invalid regression model type: Choose 'random_forest', 'linear_regression', or 'gradient_boosting'.\")\n    elif learning_task_type == 'classification':\n        if model_type == 'random_forest':\n            model = RandomForestClassifier(**kwargs)\n        elif model_type == 'gradient_boosting':\n            model = GradientBoostingClassifier(**kwargs)\n        else:\n            raise ValueError(\"Invalid classification model type: Choose 'random_forest' or 'gradient_boosting'.\")\n    else:\n        raise ValueError(\"Learning types: ['classification','regression'] \\n Model types: ['linear_regression','random_forest','gradient_boosting']\")\n    model.fit(X_train, y_train)\n    return model\n\n\ndef tune(model, X_train, y_train, param_grid, scoring=None):\n    \"\"\"Performs hyper-parameter tuning.\"\"\"\n    grid_search = GridSearchCV(model, param_grid, scoring=scoring, cv=5, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    model = grid_search.best_estimator_\n    print(f\"Best params: {grid_search.best_params_}\")\n    return model\n\n\ndef validation_eval(learning_task_type, model, X_val, y_val):\n    y_pred = model.predict(X_val)\n\n    if learning_task_type == 'regression':\n        MAE = mean_absolute_error(y_val, y_pred)\n        RMSE = np.sqrt(mean_squared_error(y_val, y_pred))\n        R2 = r2_score(y_val, y_pred)\n\n        print(f\"Mean Absolute Error (MAE): {MAE:.4f}\")\n        print(f\"Root Mean Squared Error (RMSE): {RMSE:.4f}\")\n        print(f\"R-squared: {R2:.4f}\")\n    \n    elif learning_task_type == 'classification':\n        cm = confusion_matrix(y_val, y_pred)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.xlabel('Predicted Labels')\n        plt.ylabel('True Labels')\n        plt.title('Confusion Matrix')\n        plt.show()\n    \n    else:\n        raise ValueError(\"Unsupported learning task type: 'regression' or 'classification'\")\n\n    return y_pred\n\n\ndef evaluate_test(learning_task_type, model, X_test, y_test):\n    \"\"\"Evaluates the model on new data or the test set.\"\"\"\n    y_pred = model.predict(X_test)\n    \n    if learning_task_type == 'regression':\n        MAE = mean_absolute_error(y_test, y_pred)\n        RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n        R2 = r2_score(y_test, y_pred)\n\n        print(f\"Mean Absolute Error (MAE): {MAE:.4f}\")\n        print(f\"Root Mean Squared Error (RMSE): {RMSE:.4f}\")\n        print(f\"R-squared: {R2:.4f}\")\n        \n    elif learning_task_type == 'classification':\n        print(\"Classification Report:\", classification_report(y_test, y_pred))\n    else:\n        raise ValueError(\"Unsupported learning task type: 'regression' or 'classification'\")\n    \n    return y_pred\n\n\nrf_param_grid = {\n    'n_estimators': [50, 100, 200],  \n    'max_depth': [5, 15, 20],  \n    'max_features': ['sqrt', 'log2'],  \n    'bootstrap': [True, False] \n}\n\nboost_param_grid = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'n_estimators': [50, 100, 200],  \n    'max_depth': [3, 5, 10], \n    'max_features': ['sqrt', 'log2']  \n}\n\n\nrf_param_grid_1 = {\n    'n_estimators': [100, 200, 500, 1000, 1500],\n    'max_depth': [None, 10, 20, 30, 50],\n    'min_samples_split': [2, 5, 10, 15],\n    'min_samples_leaf': [1, 2, 4, 8],\n    'max_features': ['sqrt', 'log2', None],\n    'bootstrap': [True, False],\n    'max_samples': [0.5, 0.75, 1.0]  # Fraction of samples for bootstrapping\n}\n\ngb_param_grid_1 = {\n    'n_estimators': [100, 300, 500, 1000, 1500],\n    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n    'max_depth': [3, 5, 10, 20],\n    'min_samples_split': [2, 5, 10, 20],\n    'min_samples_leaf': [1, 2, 4, 8],\n    'subsample': [0.6, 0.8, 1.0],  # Fraction of samples used for each tree\n    'max_features': ['sqrt', 'log2', None]\n}\n\n\nsubset = df_clean[['Average Grade 8 Proficiency',\n    'Metric Value - College Persistence',\n    'Supportive Environment - Element Score',\n    'Graduation Rate',\n    'Metric Value - Average Regents Score - English',\n    'Metric Value - College and Career Preparatory Course Index',\n    'Percent HRA Eligible',\n    'Student Percent - Black and Hispanic',\n    'Percent Students with IEPs',\n    'dropout_rate',\n    'Percent in Temp Housing',\n    'Percent Overage / Undercredited',\n    'Percent of Students Chronically Absent'\n]]\n\n\nsubset['log_dropout_rate'] = np.log(subset['dropout_rate'] + 1)\nsubset['log_temp_housing'] = np.log(subset['Percent in Temp Housing'] + 1)\nsubset['log_percent_overage'] = np.log(subset['Percent Overage / Undercredited'] + 1)\n\n/var/folders/hl/7yl66vtn2mn30d649bqpv1wc0000gn/T/ipykernel_85734/3082819323.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset['log_dropout_rate'] = np.log(subset['dropout_rate'] + 1)\n/var/folders/hl/7yl66vtn2mn30d649bqpv1wc0000gn/T/ipykernel_85734/3082819323.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset['log_temp_housing'] = np.log(subset['Percent in Temp Housing'] + 1)\n/var/folders/hl/7yl66vtn2mn30d649bqpv1wc0000gn/T/ipykernel_85734/3082819323.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset['log_percent_overage'] = np.log(subset['Percent Overage / Undercredited'] + 1)\n\n\n\nsubset2 = subset[['Average Grade 8 Proficiency',\n    'Metric Value - College Persistence',\n    'Supportive Environment - Element Score',\n    'Graduation Rate',\n    'Metric Value - Average Regents Score - English',\n    'Metric Value - College and Career Preparatory Course Index',\n    'Percent HRA Eligible',\n    'Student Percent - Black and Hispanic',\n    'Percent Students with IEPs',\n    'Percent of Students Chronically Absent',\n    'log_dropout_rate', 'log_temp_housing','log_percent_overage'\n]]\n\n\n\n\nRegression is a supervised learning task used to identify relationships between continuous data. In this task, we have one variable of interest, referred to as the dependent variable, and several independent features, which we hypothesize are related to the dependent variable. The strength of these relationships is quantified by the coefficients of the independent variables.\nIn this case, our goal is to identify the drivers of chronic absenteeism. By analyzing these outputs, schools can implement targeted interventions to reduce chronic absenteeism in New York City Public Schools.\nTarget Variable: Percent of Students Chronically Absent\n\n\nLASSO, or Least Absolute Shrinkage and Selection Operator is a regularization technique known as L1 Regularization. It is a good option in this case because of its ability to work with high dimensional data. We can think of it like a rope lasso that is thrown around all the features and filters out the variables that do not contribute to our model, by pushing their coefficients to 0. It does this by introducing a regularization hyperparamter, which we will tune to help shrink the coefficicents. This will help us perform feature selection automatically, and find the importance of the terms we are left with.\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_clean, target_column='Percent of Students Chronically Absent')\n\n\nlasso = Lasso(alpha=0.01) \nlasso.fit(X_train, y_train)\n\ny_pred = lasso.predict(X_val)\n\nprint(\"Validation MSE:\", mean_squared_error(y_val, y_pred))\nprint(\"Validation R-sq:\", lasso.score(X_val, y_val))\n\nplt.scatter(y_val, y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot\")\nplt.show()\n\nValidation MSE: 0.015104396280517757\nValidation R-sq: 0.416731918115706\n\n\n\n\n\n\n\n\n\nTo tune our hyperparamter, we will use scikit-learn’s cross validation tool specifically for Lasso regression, LassoCV to find the optimal choice for alpha\n\nlasso_cv = LassoCV(alphas=[0.1, 0.01, 0.05, 0.001, 0.005, 0.0005, 0.0001], cv=5)\nlasso_cv.fit(X_train, y_train)\nprint(f\"Optimal alpha: {lasso_cv.alpha_}\")\n\nOptimal alpha: 0.005\n\n\n\n# input tuned alpha and evalate using test data\nlasso = Lasso(alpha=0.005)\nlasso.fit(X_train, y_train)\n\n# get evaluation scores after tuning\ny_pred = lasso.predict(X_test)\n\nprint(\"Evaluation MSE:\", mean_squared_error(y_test, y_pred))\nprint(\"Evaluation R-sq:\", lasso.score(X_test, y_test))\n\nplt.scatter(y_test, y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot\")\nplt.show()\n\nEvaluation MSE: 0.009902842233027655\nEvaluation R-sq: 0.6417283654733892\n\n\n\n\n\n\n\n\n\nOur model fit has improved to 64% and our MSE has decreased using our tuned hyperparameter.\nNow, using the absolute value of the coefficients, we can visualize the features that are most relevant to our target variable\n\nfeature_names = X.columns\ncoefs = lasso.coef_\n\ncoefs_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefs})\n\n\ncoefs_df['abs_val_coef'] = coefs_df['Coefficient'].abs()\ncoefs_df = coefs_df.sort_values(by='abs_val_coef', ascending=False)\n\ncoefs_df = coefs_df.loc[coefs_df['abs_val_coef']&gt;0]\n\nplt.figure(figsize=(10, 6))\ncoefs_df.plot(kind='barh', x='Feature', y='abs_val_coef', legend=False)\nplt.xlabel('Absolute Value of Coefficient')\nplt.title('Feature Importance Based on Lasso Regression')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest is a supervised learning task that is an ensemble of many decision trees. Simply, decision trees work by starting at a root node and split depending on questions asked at each successive node. Random Forest is just many decision trees working together. It is superior to single decision trees because it prevents overfitting by averaging the outcome of the individual trees to get the final result. In addition, it is less sensitive to multicollinearity and normalization than other regression methods and can learn non-linear boundaries.\nThere are a few important hyperparamters to tune in random forest regression. This includes n_estimators (the number of trees in the forest), max_depth (the maximum depth or number of splits each tree can have), max_features (how many features are considered for splits), and bootstrap (whether bootstrapping (resampling with replacement) should be used) (https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\nWhen bootstrapping is used, this is considered a bagging method (Bootstrap Aggregating).\nWe will apply the Random Forest Regressor and evaluate whether it improves our results.\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df_regression, 'Percent of Students Chronically Absent')\n\n\nmodel = train(X_train, y_train, 'regression', 'random_forest')\ny_pred = validation_eval('regression',model,  X_val, y_val)\n\nMean Absolute Error (MAE): 0.0836\nRoot Mean Squared Error (RMSE): 0.1092\nR-squared: 0.5426\n\n\n\nplt.scatter(y_val, y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nTune and Retrain Model\n\ntune(model, X_train, y_train, rf_param_grid, scoring='neg_mean_squared_error')\n\nBest params: {'bootstrap': True, 'max_depth': 20, 'max_features': 'log2', 'n_estimators': 50}\n\n\nRandomForestRegressor(max_depth=20, max_features='log2', n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(max_depth=20, max_features='log2', n_estimators=50) \n\n\n\nmodel = train(X_train, y_train, 'regression', 'random_forest', bootstrap = True, max_depth= 20, max_features= 'log2', n_estimators= 50)\n\n\ny_pred = evaluate_test('regression', model, X_test, y_test)\n\nMean Absolute Error (MAE): 0.0858\nRoot Mean Squared Error (RMSE): 0.1082\nR-squared: 0.5566\n\n\n\nplt.scatter(y_test, y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nA handy output of Random Forest is feature importance. Now we can see which features are most important to our target variable.\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\n\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n\n\nplt.figure(figsize=(10, 6))\nimportance_df.plot(kind='barh', x='Feature', y='Importance', legend=False)\nplt.xlabel('Importance')\nplt.title('Feature Importance Based on Random Forest Regression')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nWe will try one more regression algorithm to see if we can improve our results.\nUnlike the previous bagging method, we will now employ a boosting technique. In boosting, trees are grown sequentially rather than in parallel, as in bagging. This approach uses random sampling with replacement over weighted data, enabling each tree to learn from the errors of the previous models. By aggregating these weak learners into a strong learner, boosting optimizes the overall model performance.\nThe important hyperparameters to tune in gradient boosting are learning rate (controls for the contribution of each tree), n_estimators (number of trees), max_depth (controls number of nodes in the tree), and max_features (number of features to consider for splits) (https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df_regression, 'Percent of Students Chronically Absent')\n\n\nmodel = train(X_train, y_train, 'regression', 'gradient_boosting')\ny_pred = validation_eval('regression', model,  X_val, y_val)\n\nMean Absolute Error (MAE): 0.0880\nRoot Mean Squared Error (RMSE): 0.1219\nR-squared: 0.4108\n\n\n\nmodel = tune(model, X_train, y_train, boost_param_grid, scoring='neg_mean_squared_error')\n\nBest params: {'learning_rate': 0.05, 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 50}\n\n\n\nmodel = train(X_train, y_train, 'regression', 'gradient_boosting', learning_rate= 0.05, max_depth= 5, max_features= 'log2', n_estimators= 50)\ny_pred = validation_eval('regression', model,  X_val, y_val)\n\nMean Absolute Error (MAE): 0.0907\nRoot Mean Squared Error (RMSE): 0.1187\nR-squared: 0.4407\n\n\n\ny_pred = evaluate_test('regression', model, X_test, y_test)\n\nMean Absolute Error (MAE): 0.0886\nRoot Mean Squared Error (RMSE): 0.1117\nR-squared: 0.5151\n\n\nOur tuned gradient boosting model performed better, but not as well as our LASSO and random forest models.\n\n\n\n\nBinary classification is a supervised learning algorithm used for predicting categorical variables with two possible classes. The goal is to train a model that accurately predicts the correct class labels as much as possible. The output of a classification task is best evaluated using a confusion matrix, which provides insights into the model’s performance. From the confusion matrix, we can calculate key metrics such as accuracy, precision, recall, and F1 score. The choice of the metric to maximize depends on the specific classification problem.\nThe calculations are as follows: \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\] \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\] \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\] F1 is the harmonic balance of precision and recall, calculated as: \\[\n\\text{F1} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nTarget variable: High Schools with high vs. low dropout risk\nIn this case, accuracy is not an ideal metric because our classes are imbalanced. The worst-case scenario is failing to identify a school as having high dropout risk. If we treat the positive class as “high schools with high dropout risk,” we aim to minimize false negatives (Type II errors) and maximize the recall score. To achieve this, we will use cross-validation to identify the algorithm that performs best for our objective.\nFirst, we will create a binary variable for high dropout risk. High-risk schools will be defined as those in the 75th percentile or above of the dropout rate distribution. Additionally, we will exclude the “Graduation Rate” feature to avoid data leakage.\n\nprint(df_clean['dropout_rate'].describe())\n\ncount    501.000000\nmean       5.358034\nstd        5.261403\nmin        0.000000\n25%        1.225000\n50%        3.700000\n75%        7.700000\nmax       27.650000\nName: dropout_rate, dtype: float64\n\n\n\ndf_class = df_clean.copy()\ndf_class['high_dropout'] = [1 if x &gt;= 7.7 else 0 for x in df_class['dropout_rate']]\ndf_class.drop(['dropout_rate','Graduation Rate'], axis=1, inplace=True)\n\n\n\nWe will begin by using a Random Forest classifier. This method is similar to the Random Forest regressor we explored earlier, but instead of predicting continuous variables, the trees now predict the class to which the data belongs. In Random Forest classification, the final prediction is determined by a majority vote among all the decision trees in the ensemble, rather than taking the average as in regression.\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_class, target_column='high_dropout')\n\n\nmodel = train(X_train, y_train, 'classification', 'random_forest')\ny_pred = validation_eval('classification',model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n           0       0.89      0.94      0.91        80\n           1       0.71      0.57      0.63        21\n\n    accuracy                           0.86       101\n   macro avg       0.80      0.75      0.77       101\nweighted avg       0.85      0.86      0.86       101\n\n\n\n\nmodel = tune(model, X_train, y_train, rf_param_grid, scoring='f1')\n\nBest params: {'bootstrap': False, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 50}\n\n\n\nmodel = train(X_train, y_train, 'classification', 'random_forest', bootstrap=True, max_depth=5, max_features='sqrt', n_estimators=50)\ny_pred = validation_eval('classification', model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n           0       0.88      0.90      0.89        80\n           1       0.58      0.52      0.55        21\n\n    accuracy                           0.82       101\n   macro avg       0.73      0.71      0.72       101\nweighted avg       0.82      0.82      0.82       101\n\n\n\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\n\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False).head(15)\n\nplt.figure(figsize=(10, 6))\nimportance_df.plot(kind='barh', x='Feature', y='Importance', legend=False)\nplt.xlabel('Importance')\nplt.title('Top 15 Features')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we will use a Gradient Boosting Classifier. This method is similar to the Gradient Boosting Regressor we tried earlier, but instead of predicting continuous values, the trees are trained to classify the data into predefined classes.\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_class, target_column='high_dropout')\n\n\nmodel = train(X_train, y_train, 'classification', 'gradient_boosting')\ny_pred = validation_eval('classification',model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n           0       0.87      0.91      0.89        75\n           1       0.70      0.62      0.65        26\n\n    accuracy                           0.83       101\n   macro avg       0.78      0.76      0.77       101\nweighted avg       0.83      0.83      0.83       101\n\n\n\n\nmodel = tune(model, X_train, y_train, boost_param_grid, scoring='f1')\n\nBest params: {'learning_rate': 0.2, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 50}\n\n\n\nmodel = train(X_train, y_train, 'classification', 'gradient_boosting', learning_rate =0.2, max_depth=5, max_features='sqrt', n_estimators=50)\ny_pred = validation_eval('classification', model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n           0       0.88      0.93      0.90        75\n           1       0.76      0.62      0.68        26\n\n    accuracy                           0.85       101\n   macro avg       0.82      0.77      0.79       101\nweighted avg       0.85      0.85      0.85       101\n\n\n\nThe gradient boosting model is a bit more accurate among the binary classifiers, it is able to predict whether a high school is high or low dropout risk with 85% accuracy\n\n\n\n\nMulticlass classification is similar to binary classification, but instead of predicting between two classes, we are now predicting across three or more classes. In this case, we are interested in classifying high schools based on their rates of college persistence, which will be categorized as low, medium, or high.\nIn our binary classification, we focused on identifying high schools with a high dropout risk rate. However, even if a high school is deemed “safe” with low dropout risk, we still want to intervene if the school has low college persistence rates. While preventing high school dropout is a significant achievement, we also want these efforts to translate into preventing dropouts from higher education.\nTarget Variable: College Persistence\nFirst, we will divide the college persistence distribution into three bins, labeling them as Low, Medium, and High.\n\ndf_multi = df_clean.copy()\ndf_multi['college_persistence_ranking'] = pd.cut(df_multi['Metric Value - College Persistence'], bins=3, labels=['Low','Medium','High'])\ndf_multi.drop(['Metric Value - College Persistence'], axis=1, inplace=True)\n\n\n\nWe will test how well a Random Forest Classifier predicts our labels, this time for three classes\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_multi, target_column='college_persistence_ranking')\n\n\nmodel = train(X_train, y_train, 'classification', 'random_forest')\ny_pred = validation_eval('classification',model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n        High       0.64      0.45      0.53        20\n         Low       0.81      0.85      0.83        26\n      Medium       0.75      0.82      0.78        55\n\n    accuracy                           0.75       101\n   macro avg       0.74      0.70      0.71       101\nweighted avg       0.75      0.75      0.74       101\n\n\n\n\nmodel = tune(model, X_train, y_train, rf_param_grid)\n\nBest params: {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 200}\n\n\n\nmodel = train(X_train, y_train, 'classification', 'random_forest', bootstrap=True, max_depth=20, max_features='sqrt', n_estimators=200)\ny_pred = validation_eval('classification', model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n        High       0.83      0.50      0.62        20\n         Low       0.74      0.77      0.75        26\n      Medium       0.74      0.84      0.79        55\n\n    accuracy                           0.75       101\n   macro avg       0.77      0.70      0.72       101\nweighted avg       0.76      0.75      0.75       101\n\n\n\nThis model can correctly classify a high school with low college persistence with 74% accuracy\n\n\n\nNow we will attempt a Gradient Boosting Classifier and see if it improves upon the Random Forest Classifier\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_multi, target_column='college_persistence_ranking')\n\n\nmodel = train(X_train, y_train, 'classification', 'gradient_boosting')\ny_pred = validation_eval('classification',model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n        High       0.85      0.71      0.77        24\n         Low       0.65      0.74      0.69        23\n      Medium       0.76      0.78      0.77        54\n\n    accuracy                           0.75       101\n   macro avg       0.76      0.74      0.75       101\nweighted avg       0.76      0.75      0.75       101\n\n\n\n\nmodel = tune(model, X_train, y_train, boost_param_grid)\n\nBest params: {'learning_rate': 0.2, 'max_depth': 3, 'max_features': 'log2', 'n_estimators': 50}\n\n\n\nmodel = train(X_train, y_train, 'classification', 'gradient_boosting', learning_rate = 0.2, max_depth=3, max_features='log2', n_estimators=50)\ny_pred = validation_eval('classification', model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n        High       0.77      0.71      0.74        24\n         Low       0.79      0.83      0.81        23\n      Medium       0.80      0.81      0.81        54\n\n    accuracy                           0.79       101\n   macro avg       0.79      0.78      0.78       101\nweighted avg       0.79      0.79      0.79       101\n\n\n\nThis model can correctly classify a high school with low college persistence with 79% accuracy, an improvement from our Random Forest Classifier"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis.\n\nIn this section, the goal is to build machine learning models to investigate the drivers of our target variables: chronic absenteeism, dropout risk, and college persistence. We will create three different models: a regression, a binary classification, and a multivariate classification. For each, we will test various algorithms and identify the one that performs best.\nI developed several functions to streamline the modeling process. These functions handle preprocessing, training, tuning, scoring the validation set, and scoring the test set.\n\nPreprocessing: This function takes a dataframe, target column, and test size as inputs. It splits the data into training, validation, and test sets. The training set is used to train the models, the validation set is crucial for tuning hyperparameters, and the test set evaluates the model’s performance on unseen data.\nTrain: This function accepts X_train, y_train, learning_task_type, model_type, and kwargs. The learning_task_type should be either ‘classification’ or ‘regression’, while model_type specifies the algorithm to be used. The kwargs parameter allows additional arguments to be passed to the algorithm, which is used once the model is tuned.\nTune: This function takes a model, X_train, y_train, param_grid, and an optional scoring parameter. It uses cross-validation to tune hyperparameters. Predefined parameter grids for random forest and gradient boosting models allow the function to perform a grid search over specified values to identify optimal parameters. The scoring parameter specifies the metric to maximize, depending on the task.\nValidation Scoring: This function evaluates the model’s performance on the validation set to guide hyperparameter tuning. It accepts learning_task_type, model, X_val, and y_val as inputs. For regression tasks, it outputs metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R². For classification tasks, it provides a confusion matrix.\nEvaluate Test: This is the final step, where the model’s performance is evaluated on the test set. The function accepts learning_task_type, model, X_test, and y_test as inputs. It outputs relevant regression or classification metrics for the tuned mode\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn import datasets, linear_model\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import Lasso, LassoCV\n\nimport statsmodels.api as sm\nfrom scipy import stats\n\n\ndf = pd.read_csv('../../data/processed-data/processed_df.csv', index_col=None)\ndf_regression = pd.read_csv('../../data/processed-data/df_subset_regression.csv', index_col=None)\n\n\n# Drop categorical and identifier columns we dont need\ndf.drop(columns=['DBN', 'School Name', 'District', 'zip code', 'Borough'], inplace=True)\n#drop columns that cause perfect multicollinearity\ndf_clean = df.drop(['Median Household Income',\"Percent Bachelor's Degree or Higher (25+)\",'Percent No High School (25+)','Student Percent - Other', 'Percent Female','Percent College Ready based on SAT Math'], axis=1)\n\n\ndef preprocessing(df, target_column, test_size=0.2):\n       \"\"\"Splits the data into training and test sets.\"\"\"\n       X = df.drop(columns=[target_column])\n       y = df[target_column]\n       #split into training, validation, test set\n       X_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=test_size, shuffle=True)\n       X_train, X_val, y_train, y_val = train_test_split(X_train1, y_train1, test_size=test_size, shuffle=True)\n       #standardize\n       scaler = StandardScaler()\n       X_train = scaler.fit_transform(X_train)\n       X_val = scaler.transform(X_val)\n       X_test = scaler.transform(X_test)\n       return X, y, X_train, X_val, X_test, y_train, y_val, y_test\n\n\ndef train(X_train, y_train, learning_task_type, model_type, **kwargs):\n    \"\"\"Trains a machine learning model.\"\"\"\n    if learning_task_type == 'regression':\n        if model_type == 'random_forest':\n            model = RandomForestRegressor(**kwargs)\n        elif model_type == 'linear_regression':\n            model = LinearRegression(**kwargs)\n        elif model_type == 'gradient_boosting':\n            model = GradientBoostingRegressor(**kwargs)\n        else:\n            raise ValueError(\"Invalid regression model type: Choose 'random_forest', 'linear_regression', or 'gradient_boosting'.\")\n    elif learning_task_type == 'classification':\n        if model_type == 'random_forest':\n            model = RandomForestClassifier(**kwargs)\n        elif model_type == 'gradient_boosting':\n            model = GradientBoostingClassifier(**kwargs)\n        else:\n            raise ValueError(\"Invalid classification model type: Choose 'random_forest' or 'gradient_boosting'.\")\n    else:\n        raise ValueError(\"Learning types: ['classification','regression'] \\n Model types: ['linear_regression','random_forest','gradient_boosting']\")\n    model.fit(X_train, y_train)\n    return model\n\n\ndef tune(model, X_train, y_train, param_grid, scoring=None):\n    \"\"\"Performs hyper-parameter tuning.\"\"\"\n    grid_search = GridSearchCV(model, param_grid, scoring=scoring, cv=5, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    model = grid_search.best_estimator_\n    print(f\"Best params: {grid_search.best_params_}\")\n    return model\n\n\ndef validation_eval(learning_task_type, model, X_val, y_val):\n    y_pred = model.predict(X_val)\n\n    if learning_task_type == 'regression':\n        MAE = mean_absolute_error(y_val, y_pred)\n        RMSE = np.sqrt(mean_squared_error(y_val, y_pred))\n        R2 = r2_score(y_val, y_pred)\n\n        print(f\"Mean Absolute Error (MAE): {MAE:.4f}\")\n        print(f\"Root Mean Squared Error (RMSE): {RMSE:.4f}\")\n        print(f\"R-squared: {R2:.4f}\")\n    \n    elif learning_task_type == 'classification':\n        cm = confusion_matrix(y_val, y_pred)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.xlabel('Predicted Labels')\n        plt.ylabel('True Labels')\n        plt.title('Confusion Matrix')\n        plt.show()\n    \n    else:\n        raise ValueError(\"Unsupported learning task type: 'regression' or 'classification'\")\n\n    return y_pred\n\n\ndef evaluate_test(learning_task_type, model, X_test, y_test):\n    \"\"\"Evaluates the model on new data or the test set.\"\"\"\n    y_pred = model.predict(X_test)\n    \n    if learning_task_type == 'regression':\n        MAE = mean_absolute_error(y_test, y_pred)\n        RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n        R2 = r2_score(y_test, y_pred)\n\n        print(f\"Mean Absolute Error (MAE): {MAE:.4f}\")\n        print(f\"Root Mean Squared Error (RMSE): {RMSE:.4f}\")\n        print(f\"R-squared: {R2:.4f}\")\n        \n    elif learning_task_type == 'classification':\n        print(\"Classification Report:\", classification_report(y_test, y_pred))\n    else:\n        raise ValueError(\"Unsupported learning task type: 'regression' or 'classification'\")\n    \n    return y_pred\n\n\nrf_param_grid = {\n    'n_estimators': [50, 100, 200],  \n    'max_depth': [5, 15, 20],  \n    'max_features': ['sqrt', 'log2'],  \n    'bootstrap': [True, False] \n}\n\nboost_param_grid = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'n_estimators': [50, 100, 200],  \n    'max_depth': [3, 5, 10], \n    'max_features': ['sqrt', 'log2']  \n}\n\n\nrf_param_grid_1 = {\n    'n_estimators': [100, 200, 500, 1000, 1500],\n    'max_depth': [None, 10, 20, 30, 50],\n    'min_samples_split': [2, 5, 10, 15],\n    'min_samples_leaf': [1, 2, 4, 8],\n    'max_features': ['sqrt', 'log2', None],\n    'bootstrap': [True, False],\n    'max_samples': [0.5, 0.75, 1.0]  # Fraction of samples for bootstrapping\n}\n\ngb_param_grid_1 = {\n    'n_estimators': [100, 300, 500, 1000, 1500],\n    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n    'max_depth': [3, 5, 10, 20],\n    'min_samples_split': [2, 5, 10, 20],\n    'min_samples_leaf': [1, 2, 4, 8],\n    'subsample': [0.6, 0.8, 1.0],  # Fraction of samples used for each tree\n    'max_features': ['sqrt', 'log2', None]\n}\n\n\nsubset = df_clean[['Average Grade 8 Proficiency',\n    'Metric Value - College Persistence',\n    'Supportive Environment - Element Score',\n    'Graduation Rate',\n    'Metric Value - Average Regents Score - English',\n    'Metric Value - College and Career Preparatory Course Index',\n    'Percent HRA Eligible',\n    'Student Percent - Black and Hispanic',\n    'Percent Students with IEPs',\n    'dropout_rate',\n    'Percent in Temp Housing',\n    'Percent Overage / Undercredited',\n    'Percent of Students Chronically Absent'\n]]\n\n\nsubset['log_dropout_rate'] = np.log(subset['dropout_rate'] + 1)\nsubset['log_temp_housing'] = np.log(subset['Percent in Temp Housing'] + 1)\nsubset['log_percent_overage'] = np.log(subset['Percent Overage / Undercredited'] + 1)\n\n/var/folders/hl/7yl66vtn2mn30d649bqpv1wc0000gn/T/ipykernel_85734/3082819323.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset['log_dropout_rate'] = np.log(subset['dropout_rate'] + 1)\n/var/folders/hl/7yl66vtn2mn30d649bqpv1wc0000gn/T/ipykernel_85734/3082819323.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset['log_temp_housing'] = np.log(subset['Percent in Temp Housing'] + 1)\n/var/folders/hl/7yl66vtn2mn30d649bqpv1wc0000gn/T/ipykernel_85734/3082819323.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  subset['log_percent_overage'] = np.log(subset['Percent Overage / Undercredited'] + 1)\n\n\n\nsubset2 = subset[['Average Grade 8 Proficiency',\n    'Metric Value - College Persistence',\n    'Supportive Environment - Element Score',\n    'Graduation Rate',\n    'Metric Value - Average Regents Score - English',\n    'Metric Value - College and Career Preparatory Course Index',\n    'Percent HRA Eligible',\n    'Student Percent - Black and Hispanic',\n    'Percent Students with IEPs',\n    'Percent of Students Chronically Absent',\n    'log_dropout_rate', 'log_temp_housing','log_percent_overage'\n]]"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#regression",
    "href": "technical-details/supervised-learning/main.html#regression",
    "title": "Supervised Learning",
    "section": "",
    "text": "Regression is a supervised learning task used to identify relationships between continuous data. In this task, we have one variable of interest, referred to as the dependent variable, and several independent features, which we hypothesize are related to the dependent variable. The strength of these relationships is quantified by the coefficients of the independent variables.\nIn this case, our goal is to identify the drivers of chronic absenteeism. By analyzing these outputs, schools can implement targeted interventions to reduce chronic absenteeism in New York City Public Schools.\nTarget Variable: Percent of Students Chronically Absent\n\n\nLASSO, or Least Absolute Shrinkage and Selection Operator is a regularization technique known as L1 Regularization. It is a good option in this case because of its ability to work with high dimensional data. We can think of it like a rope lasso that is thrown around all the features and filters out the variables that do not contribute to our model, by pushing their coefficients to 0. It does this by introducing a regularization hyperparamter, which we will tune to help shrink the coefficicents. This will help us perform feature selection automatically, and find the importance of the terms we are left with.\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_clean, target_column='Percent of Students Chronically Absent')\n\n\nlasso = Lasso(alpha=0.01) \nlasso.fit(X_train, y_train)\n\ny_pred = lasso.predict(X_val)\n\nprint(\"Validation MSE:\", mean_squared_error(y_val, y_pred))\nprint(\"Validation R-sq:\", lasso.score(X_val, y_val))\n\nplt.scatter(y_val, y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot\")\nplt.show()\n\nValidation MSE: 0.015104396280517757\nValidation R-sq: 0.416731918115706\n\n\n\n\n\n\n\n\n\nTo tune our hyperparamter, we will use scikit-learn’s cross validation tool specifically for Lasso regression, LassoCV to find the optimal choice for alpha\n\nlasso_cv = LassoCV(alphas=[0.1, 0.01, 0.05, 0.001, 0.005, 0.0005, 0.0001], cv=5)\nlasso_cv.fit(X_train, y_train)\nprint(f\"Optimal alpha: {lasso_cv.alpha_}\")\n\nOptimal alpha: 0.005\n\n\n\n# input tuned alpha and evalate using test data\nlasso = Lasso(alpha=0.005)\nlasso.fit(X_train, y_train)\n\n# get evaluation scores after tuning\ny_pred = lasso.predict(X_test)\n\nprint(\"Evaluation MSE:\", mean_squared_error(y_test, y_pred))\nprint(\"Evaluation R-sq:\", lasso.score(X_test, y_test))\n\nplt.scatter(y_test, y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot\")\nplt.show()\n\nEvaluation MSE: 0.009902842233027655\nEvaluation R-sq: 0.6417283654733892\n\n\n\n\n\n\n\n\n\nOur model fit has improved to 64% and our MSE has decreased using our tuned hyperparameter.\nNow, using the absolute value of the coefficients, we can visualize the features that are most relevant to our target variable\n\nfeature_names = X.columns\ncoefs = lasso.coef_\n\ncoefs_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefs})\n\n\ncoefs_df['abs_val_coef'] = coefs_df['Coefficient'].abs()\ncoefs_df = coefs_df.sort_values(by='abs_val_coef', ascending=False)\n\ncoefs_df = coefs_df.loc[coefs_df['abs_val_coef']&gt;0]\n\nplt.figure(figsize=(10, 6))\ncoefs_df.plot(kind='barh', x='Feature', y='abs_val_coef', legend=False)\nplt.xlabel('Absolute Value of Coefficient')\nplt.title('Feature Importance Based on Lasso Regression')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest is a supervised learning task that is an ensemble of many decision trees. Simply, decision trees work by starting at a root node and split depending on questions asked at each successive node. Random Forest is just many decision trees working together. It is superior to single decision trees because it prevents overfitting by averaging the outcome of the individual trees to get the final result. In addition, it is less sensitive to multicollinearity and normalization than other regression methods and can learn non-linear boundaries.\nThere are a few important hyperparamters to tune in random forest regression. This includes n_estimators (the number of trees in the forest), max_depth (the maximum depth or number of splits each tree can have), max_features (how many features are considered for splits), and bootstrap (whether bootstrapping (resampling with replacement) should be used) (https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\nWhen bootstrapping is used, this is considered a bagging method (Bootstrap Aggregating).\nWe will apply the Random Forest Regressor and evaluate whether it improves our results.\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df_regression, 'Percent of Students Chronically Absent')\n\n\nmodel = train(X_train, y_train, 'regression', 'random_forest')\ny_pred = validation_eval('regression',model,  X_val, y_val)\n\nMean Absolute Error (MAE): 0.0836\nRoot Mean Squared Error (RMSE): 0.1092\nR-squared: 0.5426\n\n\n\nplt.scatter(y_val, y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nTune and Retrain Model\n\ntune(model, X_train, y_train, rf_param_grid, scoring='neg_mean_squared_error')\n\nBest params: {'bootstrap': True, 'max_depth': 20, 'max_features': 'log2', 'n_estimators': 50}\n\n\nRandomForestRegressor(max_depth=20, max_features='log2', n_estimators=50)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestRegressor?Documentation for RandomForestRegressoriFittedRandomForestRegressor(max_depth=20, max_features='log2', n_estimators=50) \n\n\n\nmodel = train(X_train, y_train, 'regression', 'random_forest', bootstrap = True, max_depth= 20, max_features= 'log2', n_estimators= 50)\n\n\ny_pred = evaluate_test('regression', model, X_test, y_test)\n\nMean Absolute Error (MAE): 0.0858\nRoot Mean Squared Error (RMSE): 0.1082\nR-squared: 0.5566\n\n\n\nplt.scatter(y_test, y_pred)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Parity Plot\")\nplt.show()\n\n\n\n\n\n\n\n\nA handy output of Random Forest is feature importance. Now we can see which features are most important to our target variable.\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\n\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False)\n\n\nplt.figure(figsize=(10, 6))\nimportance_df.plot(kind='barh', x='Feature', y='Importance', legend=False)\nplt.xlabel('Importance')\nplt.title('Feature Importance Based on Random Forest Regression')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nWe will try one more regression algorithm to see if we can improve our results.\nUnlike the previous bagging method, we will now employ a boosting technique. In boosting, trees are grown sequentially rather than in parallel, as in bagging. This approach uses random sampling with replacement over weighted data, enabling each tree to learn from the errors of the previous models. By aggregating these weak learners into a strong learner, boosting optimizes the overall model performance.\nThe important hyperparameters to tune in gradient boosting are learning rate (controls for the contribution of each tree), n_estimators (number of trees), max_depth (controls number of nodes in the tree), and max_features (number of features to consider for splits) (https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df_regression, 'Percent of Students Chronically Absent')\n\n\nmodel = train(X_train, y_train, 'regression', 'gradient_boosting')\ny_pred = validation_eval('regression', model,  X_val, y_val)\n\nMean Absolute Error (MAE): 0.0880\nRoot Mean Squared Error (RMSE): 0.1219\nR-squared: 0.4108\n\n\n\nmodel = tune(model, X_train, y_train, boost_param_grid, scoring='neg_mean_squared_error')\n\nBest params: {'learning_rate': 0.05, 'max_depth': 5, 'max_features': 'log2', 'n_estimators': 50}\n\n\n\nmodel = train(X_train, y_train, 'regression', 'gradient_boosting', learning_rate= 0.05, max_depth= 5, max_features= 'log2', n_estimators= 50)\ny_pred = validation_eval('regression', model,  X_val, y_val)\n\nMean Absolute Error (MAE): 0.0907\nRoot Mean Squared Error (RMSE): 0.1187\nR-squared: 0.4407\n\n\n\ny_pred = evaluate_test('regression', model, X_test, y_test)\n\nMean Absolute Error (MAE): 0.0886\nRoot Mean Squared Error (RMSE): 0.1117\nR-squared: 0.5151\n\n\nOur tuned gradient boosting model performed better, but not as well as our LASSO and random forest models."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#binary-classification",
    "href": "technical-details/supervised-learning/main.html#binary-classification",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary classification is a supervised learning algorithm used for predicting categorical variables with two possible classes. The goal is to train a model that accurately predicts the correct class labels as much as possible. The output of a classification task is best evaluated using a confusion matrix, which provides insights into the model’s performance. From the confusion matrix, we can calculate key metrics such as accuracy, precision, recall, and F1 score. The choice of the metric to maximize depends on the specific classification problem.\nThe calculations are as follows: \\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\] \\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\] \\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\] F1 is the harmonic balance of precision and recall, calculated as: \\[\n\\text{F1} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nTarget variable: High Schools with high vs. low dropout risk\nIn this case, accuracy is not an ideal metric because our classes are imbalanced. The worst-case scenario is failing to identify a school as having high dropout risk. If we treat the positive class as “high schools with high dropout risk,” we aim to minimize false negatives (Type II errors) and maximize the recall score. To achieve this, we will use cross-validation to identify the algorithm that performs best for our objective.\nFirst, we will create a binary variable for high dropout risk. High-risk schools will be defined as those in the 75th percentile or above of the dropout rate distribution. Additionally, we will exclude the “Graduation Rate” feature to avoid data leakage.\n\nprint(df_clean['dropout_rate'].describe())\n\ncount    501.000000\nmean       5.358034\nstd        5.261403\nmin        0.000000\n25%        1.225000\n50%        3.700000\n75%        7.700000\nmax       27.650000\nName: dropout_rate, dtype: float64\n\n\n\ndf_class = df_clean.copy()\ndf_class['high_dropout'] = [1 if x &gt;= 7.7 else 0 for x in df_class['dropout_rate']]\ndf_class.drop(['dropout_rate','Graduation Rate'], axis=1, inplace=True)\n\n\n\nWe will begin by using a Random Forest classifier. This method is similar to the Random Forest regressor we explored earlier, but instead of predicting continuous variables, the trees now predict the class to which the data belongs. In Random Forest classification, the final prediction is determined by a majority vote among all the decision trees in the ensemble, rather than taking the average as in regression.\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_class, target_column='high_dropout')\n\n\nmodel = train(X_train, y_train, 'classification', 'random_forest')\ny_pred = validation_eval('classification',model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n           0       0.89      0.94      0.91        80\n           1       0.71      0.57      0.63        21\n\n    accuracy                           0.86       101\n   macro avg       0.80      0.75      0.77       101\nweighted avg       0.85      0.86      0.86       101\n\n\n\n\nmodel = tune(model, X_train, y_train, rf_param_grid, scoring='f1')\n\nBest params: {'bootstrap': False, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 50}\n\n\n\nmodel = train(X_train, y_train, 'classification', 'random_forest', bootstrap=True, max_depth=5, max_features='sqrt', n_estimators=50)\ny_pred = validation_eval('classification', model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n           0       0.88      0.90      0.89        80\n           1       0.58      0.52      0.55        21\n\n    accuracy                           0.82       101\n   macro avg       0.73      0.71      0.72       101\nweighted avg       0.82      0.82      0.82       101\n\n\n\n\nfeature_importances = model.feature_importances_\nfeature_names = X.columns\n\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances}).sort_values(by='Importance', ascending=False).head(15)\n\nplt.figure(figsize=(10, 6))\nimportance_df.plot(kind='barh', x='Feature', y='Importance', legend=False)\nplt.xlabel('Importance')\nplt.title('Top 15 Features')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we will use a Gradient Boosting Classifier. This method is similar to the Gradient Boosting Regressor we tried earlier, but instead of predicting continuous values, the trees are trained to classify the data into predefined classes.\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_class, target_column='high_dropout')\n\n\nmodel = train(X_train, y_train, 'classification', 'gradient_boosting')\ny_pred = validation_eval('classification',model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n           0       0.87      0.91      0.89        75\n           1       0.70      0.62      0.65        26\n\n    accuracy                           0.83       101\n   macro avg       0.78      0.76      0.77       101\nweighted avg       0.83      0.83      0.83       101\n\n\n\n\nmodel = tune(model, X_train, y_train, boost_param_grid, scoring='f1')\n\nBest params: {'learning_rate': 0.2, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 50}\n\n\n\nmodel = train(X_train, y_train, 'classification', 'gradient_boosting', learning_rate =0.2, max_depth=5, max_features='sqrt', n_estimators=50)\ny_pred = validation_eval('classification', model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n           0       0.88      0.93      0.90        75\n           1       0.76      0.62      0.68        26\n\n    accuracy                           0.85       101\n   macro avg       0.82      0.77      0.79       101\nweighted avg       0.85      0.85      0.85       101\n\n\n\nThe gradient boosting model is a bit more accurate among the binary classifiers, it is able to predict whether a high school is high or low dropout risk with 85% accuracy"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#multiclass-classification",
    "href": "technical-details/supervised-learning/main.html#multiclass-classification",
    "title": "Supervised Learning",
    "section": "",
    "text": "Multiclass classification is similar to binary classification, but instead of predicting between two classes, we are now predicting across three or more classes. In this case, we are interested in classifying high schools based on their rates of college persistence, which will be categorized as low, medium, or high.\nIn our binary classification, we focused on identifying high schools with a high dropout risk rate. However, even if a high school is deemed “safe” with low dropout risk, we still want to intervene if the school has low college persistence rates. While preventing high school dropout is a significant achievement, we also want these efforts to translate into preventing dropouts from higher education.\nTarget Variable: College Persistence\nFirst, we will divide the college persistence distribution into three bins, labeling them as Low, Medium, and High.\n\ndf_multi = df_clean.copy()\ndf_multi['college_persistence_ranking'] = pd.cut(df_multi['Metric Value - College Persistence'], bins=3, labels=['Low','Medium','High'])\ndf_multi.drop(['Metric Value - College Persistence'], axis=1, inplace=True)\n\n\n\nWe will test how well a Random Forest Classifier predicts our labels, this time for three classes\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_multi, target_column='college_persistence_ranking')\n\n\nmodel = train(X_train, y_train, 'classification', 'random_forest')\ny_pred = validation_eval('classification',model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n        High       0.64      0.45      0.53        20\n         Low       0.81      0.85      0.83        26\n      Medium       0.75      0.82      0.78        55\n\n    accuracy                           0.75       101\n   macro avg       0.74      0.70      0.71       101\nweighted avg       0.75      0.75      0.74       101\n\n\n\n\nmodel = tune(model, X_train, y_train, rf_param_grid)\n\nBest params: {'bootstrap': True, 'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 200}\n\n\n\nmodel = train(X_train, y_train, 'classification', 'random_forest', bootstrap=True, max_depth=20, max_features='sqrt', n_estimators=200)\ny_pred = validation_eval('classification', model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n        High       0.83      0.50      0.62        20\n         Low       0.74      0.77      0.75        26\n      Medium       0.74      0.84      0.79        55\n\n    accuracy                           0.75       101\n   macro avg       0.77      0.70      0.72       101\nweighted avg       0.76      0.75      0.75       101\n\n\n\nThis model can correctly classify a high school with low college persistence with 74% accuracy\n\n\n\nNow we will attempt a Gradient Boosting Classifier and see if it improves upon the Random Forest Classifier\n\nX, y, X_train, X_val, X_test, y_train, y_val, y_test = preprocessing(df = df_multi, target_column='college_persistence_ranking')\n\n\nmodel = train(X_train, y_train, 'classification', 'gradient_boosting')\ny_pred = validation_eval('classification',model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n        High       0.85      0.71      0.77        24\n         Low       0.65      0.74      0.69        23\n      Medium       0.76      0.78      0.77        54\n\n    accuracy                           0.75       101\n   macro avg       0.76      0.74      0.75       101\nweighted avg       0.76      0.75      0.75       101\n\n\n\n\nmodel = tune(model, X_train, y_train, boost_param_grid)\n\nBest params: {'learning_rate': 0.2, 'max_depth': 3, 'max_features': 'log2', 'n_estimators': 50}\n\n\n\nmodel = train(X_train, y_train, 'classification', 'gradient_boosting', learning_rate = 0.2, max_depth=3, max_features='log2', n_estimators=50)\ny_pred = validation_eval('classification', model,  X_val, y_val)\n\n\n\n\n\n\n\n\n\ny_pred = evaluate_test('classification', model, X_test, y_test)\n\nClassification Report:               precision    recall  f1-score   support\n\n        High       0.77      0.71      0.74        24\n         Low       0.79      0.83      0.81        23\n      Medium       0.80      0.81      0.81        54\n\n    accuracy                           0.79       101\n   macro avg       0.79      0.78      0.78       101\nweighted avg       0.79      0.79      0.79       101\n\n\n\nThis model can correctly classify a high school with low college persistence with 79% accuracy, an improvement from our Random Forest Classifier"
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#suggested-page-structure",
    "href": "technical-details/data-collection/main.html#suggested-page-structure",
    "title": "Data Collection",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/main.html#what-to-address",
    "href": "technical-details/data-collection/main.html#what-to-address",
    "title": "Data Collection",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#start-collecting-data",
    "href": "technical-details/data-collection/main.html#start-collecting-data",
    "title": "Data Collection",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/main.html#saving-the-raw-data",
    "href": "technical-details/data-collection/main.html#saving-the-raw-data",
    "title": "Data Collection",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/main.html#requirements",
    "href": "technical-details/data-collection/main.html#requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#other-datasets-used",
    "href": "technical-details/data-collection/main.html#other-datasets-used",
    "title": "Data Collection",
    "section": "Other Datasets used",
    "text": "Other Datasets used\nThe get data on quality of each high school in New York City including some of my targets of interest inclduing chronic absenteeism and college persistence, I downloaded a dataset from NYC InfoHub.\nDownload the 2022-23 High School Quality Report\nIn addition, in order to map the quality data to the Census data, I used an NYC High School Directory file found on the Infohub site. This provided us with zip codes that I could join the Census data on.\nDownload the mapping file\n\nSummary"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\nOne major challenge faced was finding the data I needed that could be aggregaated to my dataset. The NYC high school quality data is aggregated by school name and DBN, which is a uniue new york identifier that uses the district, bourough, and nyc doe school number. However many other datasets, for example country wide ones, that I was interested in getting features from did not include these aggregations since it appears to be new york specific. Therefore I had to spend a lot of time and was a little limited to finding data with this aggregation. I was able to find a mapping of DBN to zip code, which allowed me to map the census data to my school data, incorporating a few socioeconomic factors.\nIn future work, I would be interesting is scaling this project to all of New York, or even country wide. Since I am only looking at New York City high schools, I only have about 500 rows, which is a bit of a small dataset. If this can be scaled to a larger amount of schools, I would probably be able to create a more robust dataset and models."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\nHowever, we by aggreagating data from 3 different datasets, we are able to evaluate many features that may have impacts on our target varaibles and help us evaluate how high school qualities and socioeconomic factors effect high school outcomes in New York City."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "The data cleaning phase is a crucial and often time-consuming part of any machine learning project. The dataset I am working with contains over 600 columns, which need to be narrowed down to the most relevant ones for analysis. Preparing the data carefully is essential for successful exploratory data analysis (EDA) and modeling. During the process, new issues often arise, particularly when working with large datasets where it’s impossible to anticipate every challenge upfront. This is why data cleaning is considered an iterative process that evolves as the project progresses.\nTo ensure the data is cleaned accurately, I relied on the NYC Public Schools’ School Quality Reports Educator Guide: High Schools 2022–231 as a data dictionary. This guide helped me understand the meaning behind many of the columns, allowing me to clean and organize the data effectively for future analysis.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sodapy import Socrata\nfrom scipy.stats import skew\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('future.no_silent_downcasting', True)\n\nRead in data\n\nquality_tabs = pd.read_excel('../../data/raw-data/202223-hs-sqr-results.xlsx', skiprows=3, nrows=506, sheet_name=None)\nACS_df = pd.read_csv('../../data/raw-data/ACS_data.csv')\ndropout_df = pd.read_csv('../../data/raw-data/dropout_data.csv', index_col=None)\nmapping_df = pd.read_csv('../../data/mapping/2021_DOE_High_School_Directory.csv', index_col=None)\n\nConcat 4 dicts of quality sheets, remove first row, drop duplicated columns\n\ndf_quality = pd.concat(quality_tabs.values(), axis=1)\n\ndf_quality = df_quality.iloc[1: , :]\n\ndf_quality = df_quality.loc[:, ~df_quality.T.duplicated(keep='first')]\n\nGet columns we want from mapping, add zip code to df\n\n#get columns \ndf_mapping = mapping_df[['dbn','postcode']]\ndf_mapping.rename(columns={\"dbn\": \"DBN\", \"postcode\": \"zip code\"}, inplace=True)\n#add zip code to df\ndf = df_quality.merge(df_mapping, on='DBN', how='left')\n\nFind missing zip codes and export as csv\n\nmissing_school_zips = df.loc[df['zip code'].isna()]\nmissing_school_zips[['DBN', 'School Name']].drop_duplicates().to_csv('../../data/mapping/missing_list.csv', index=False)\n\nManually impute zip codes and read back in filled df\n\nmap2 = pd.read_csv('../../data/mapping/missing_list_labeled.csv')\n\nMerge again and fill in missing zip codes\n\ndf = df.merge(map2, on='DBN', how='left')\ndf['zip code'] = df['zip code'].fillna(df['zipcode1'])\ndf.drop(columns=['school_name1', 'zipcode1'],inplace=True)\n\nAdd in ACS data\n\nACS_df['zip code tabulation area'] = pd.to_numeric(ACS_df['zip code tabulation area'])\nACS_df.rename(columns={\"zip code tabulation area\": \"zip code\"}, inplace=True)\ndf = df.merge(ACS_df, on='zip code', how='left')\n\nClean get columns we want from dropout df, filter for 2018 cohort year - which will match up with 2022-23 data, filter school and student type\n\ndropout_df = dropout_df.loc[dropout_df['cohort_year']==2018]\ndropout_df = dropout_df.loc[dropout_df['report_category'].str.contains('School')]\ndropout_df = dropout_df.loc[dropout_df['category']=='All Students']\n\ndropout_df = dropout_df[['geographic_subdivision','category','school_name','dropout_1']]\n\nCalculate mean dropout rate per school\n\ndropout_df['dropout_1'] = pd.to_numeric(dropout_df['dropout_1'], errors='coerce')\ntransformed_df = dropout_df.groupby(['geographic_subdivision','category','school_name']).transform('mean')\ndropout_df = pd.concat([dropout_df[['geographic_subdivision', 'category', 'school_name']], transformed_df], axis=1)\ndropout_df = dropout_df.drop_duplicates()\ndropout_df.rename(columns={\"dropout_1\": \"dropout_rate\", \"school_name\":\"School Name\"}, inplace=True)\ndropout_df = dropout_df[['School Name','dropout_rate']]\n\nMerge to final df\n\ndropout_df['School Name'] = dropout_df['School Name'].str.lower()\ndf['School Name'] = df['School Name'].str.lower()\ndf = df.merge(dropout_df, on='School Name', how='left')\n\nPre cleaning df.head check\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0_x\nDBN\nSchool Name\nSchool Type\nEnrollment\nRigorous Instruction Rating\nCollaborative Teachers Rating\nSupportive Environment Rating\nEffective School Leadership Rating\nStrong Family-Community Ties Rating\n...\nUnnamed: 0_y\nPercent No High School (25+)\nPercent Bachelor's Degree or Higher (25+)\nPercent Language Other Than English at Home\nPercent Population with Disabilities\nPercent Foreign-Born Population\nPercent Households with Broadband Internet\nMedian Household Income\nPercent Households on SNAP/Food Stamps\ndropout_rate\n\n\n\n\n0\nNaN\n01M292\norchard collegiate academy\nHigh School\n269.0\nMeeting Target\nExceeding Target\nMeeting Target\nMeeting Target\nApproaching Target\n...\n2578.0\n16.2\n38.1\n53.0\n13.0\n38.1\n76.1\n43362.0\n34.0\n3.3\n\n\n1\nNaN\n01M448\nuniversity neighborhood high school\nHigh School\n485.0\nExceeding Target\nExceeding Target\nExceeding Target\nExceeding Target\nMeeting Target\n...\n2578.0\n16.2\n38.1\n53.0\n13.0\n38.1\n76.1\n43362.0\n34.0\n2.9\n\n\n2\nNaN\n01M450\neast side community school\nHigh School\n389.0\nExceeding Target\nExceeding Target\nExceeding Target\nExceeding Target\nExceeding Target\n...\n2584.0\n7.0\n62.3\n34.6\n14.0\n24.3\n85.3\n83344.0\n18.0\n0.0\n\n\n3\nNaN\n01M539\nnew explorations into science, technology and ...\nHigh School\n620.0\nMeeting Target\nMeeting Target\nExceeding Target\nExceeding Target\nExceeding Target\n...\n2578.0\n16.2\n38.1\n53.0\n13.0\n38.1\n76.1\n43362.0\n34.0\n0.7\n\n\n4\nNaN\n01M696\nbard high school early college\nHigh School\n565.0\nExceeding Target\nApproaching Target\nMeeting Target\nMeeting Target\nMeeting Target\n...\n2578.0\n16.2\n38.1\n53.0\n13.0\n38.1\n76.1\n43362.0\n34.0\n0.0\n\n\n\n\n5 rows × 555 columns\n\n\n\nPre Cleaning data types\n\ndf.dtypes.value_counts()\n\nobject     280\nfloat64    275\nName: count, dtype: int64\n\n\nAccording to the Educator Guide, “metrics with fewer than the minimum number of students are not reported and do not contribute to the school’s ratings because of confidentiality considerations and the unreliability of measurements based on small numbers”1. These are shown in the data as values of ‘N&lt;15’ and ‘N&lt;5’. We cannot impute 0 for these values since that would fasely discredit the school. Instead, I changed these values to NA and did not include them when processing. Similarly, we will make values that are ‘&lt;95%’ as their max 95%.\n\ndf = df.replace('N&lt;15', np.nan).infer_objects(copy=False)\ndf = df.replace('N&lt;5', np.nan).infer_objects(copy=False)\n\ndf = df.replace('&gt; 95%', int(0.95)).infer_objects(copy=False)\n\nPre cleaning missingness\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(df.isnull(), cbar=True, cmap=\"viridis\")\nplt.title(\"Missing Values Before Cleaning\")\nplt.show()\n\n\n\n\n\n\n\n\nRemove columns by string:\nUnnamed: remove NA columns\nN count / metric rating / metric score / comparison group: we just need metric value % columns, these give the same info\npositive responses / percent positive: remove borough and city positive response columns - we just need element score for these metrics\nquality review: these metrics are included in the Element score metrics\nRemove granular columns: scores by specific courses, demographics\n\nstring_check = \"Unnamed|N count|metric rating|metric score|comparison group|positive responses|percent positive|Quality Review\"\n\n#specific courses\nspecific_courses = \"Chemistry|Earth Science|Algebra|US History|Geometry|Global History|Living Environment|Credits\"\n\n#demographics\ndemographics = 'Asian|White|Native|Black|Hispanic|Male|Female|English Lang|ELL|IEPs|NYSAA'\n\ndf = df.loc[:, ~df.columns.str.contains(string_check, regex=True, case=False)]\ndf = df.loc[:, ~df.columns.str.contains(specific_courses, regex=True, case=False)]\n\ndf= df.loc[:, ~(df.columns.str.contains(demographics, regex=True, case=False) & ~(df.columns.str.contains('percent ', regex=True, case=False)))]\n\nDrop School Type (they are all High Schools) and columns with survey response questions, we have score columns for this\n\ndf = df.drop(columns=['School Type','Rigorous Instruction Rating','Collaborative Teachers Rating','Supportive Environment Rating','Effective School Leadership Rating','Strong Family-Community Ties Rating','Trust Rating','Student Achievement Rating'], axis=1)\n\n\nFeature Engineering\nAverage rates that are 4 and 6 years then drop\n\ndf['Graduation Rate'] = df[['Metric Value - 6-Year Graduation Rate - All Students', 'Metric Value - 4-Year Graduation Rate - All Students']].mean(axis=1)\ndf['% Attaining Regents Diploma'] = df[['Metric Value - % Attaining Regents Diploma (4 year)', 'Metric Value - % Attaining Regents Diploma (6 year)']].mean(axis=1)\ndf['High School Persistence Rate'] = df[['Metric Value - 6-Year High School Persistence Rate', 'Metric Value - 4-Year High School Persistence Rate']].mean(axis=1)\ndf = df.drop(columns=['Metric Value - 6-Year Graduation Rate - All Students',\n                    'Metric Value - 4-Year Graduation Rate - All Students',\n                    'Metric Value - % Attaining Regents Diploma (4 year)',\n                    'Metric Value - % Attaining Regents Diploma (6 year)',\n                    'Metric Value - 4-Year High School Persistence Rate',\n                    'Metric Value - 6-Year High School Persistence Rate'])\n\nCombine student demographic percentages - group black and hispanic since its the majority. Combine teacher demographic percentages - 2 groups: Teacher White and Teacher Other. Remove nearby student percentages. Remove Borough Percentages.\n\n#student demographic \ndf['Student Percent - Black and Hispanic'] = (df['Student Percent - Black'] + df['Student Percent - Hispanic'])\ndf['Student Percent - Other'] = df['Student Percent - Native American'] + df['Student Percent - Native Hawaiian or Pacific Islander'] + df['Student Percent - Asian'] + df['Student Percent - White']\ndf = df.drop(['Student Percent - White','Student Percent - Native American', 'Student Percent - Native Hawaiian or Pacific Islander','Student Percent - Asian', 'Student Percent - Black', 'Student Percent - Hispanic'], axis=1)\n\n#teacher demographic \ndf['Teacher - Other'] = df['Teacher Percent - Native American'] + df['Teacher Percent - Native Hawaiian or Pacific Islander'] + df['Teacher Percent - Asian'] + df['Teacher Percent - Hispanic'] + df['Teacher Percent - Black']\ndf = df.drop(['Teacher Percent - Native American', 'Teacher Percent - Native Hawaiian or Pacific Islander','Teacher Percent - Asian', 'Teacher Percent - Black', 'Teacher Percent - Hispanic'], axis=1)\n\n#nearby student percentages\ndf = df.drop(['Nearby Student Percent - White','Nearby Student Percent - Native American', 'Nearby Student Percent - Native Hawaiian or Pacific Islander','Nearby Student Percent - Asian', 'Nearby Student Percent - Black', 'Nearby Student Percent - Hispanic'], axis=1)\n\n#Borough Percentages\ndf = df.drop(['Borough Percent - White','Borough Percent - Native American', 'Borough Percent - Native Hawaiian or Pacific Islander','Borough Percent - Asian', 'Borough Percent - Black', 'Borough Percent - Hispanic'], axis=1)\n\nOnly keep general postsecondary enrollment and graduation columns\n\ndf = df.loc[:, ~((df.columns.str.contains(\"postsecondary enrollment\", case=False)) & ~(df.columns == \"Metric Value - Postsecondary Enrollment Rate\"))]\ndf= df.loc[:, ~(df.columns.str.contains('graduated|graduation', regex=True, case=False) & ~(df.columns == \"Graduation Rate\"))]\n\nExtract District and Borough from DBN column and map borough name to value\nDBN: (##(District)X(Borough)###(School ID))\n\ndf = df.copy()\ndf['District'] = df['DBN'].str[0:2]\ndf['Borough'] = df['DBN'].str[2:3]\n\nborough_map = {'M': 'Manhattan', 'K': 'Brooklyn', 'X': 'Bronx', 'Q':'Queens', 'R':'Staten Island'}\ndf['Borough'] = df['Borough'].map(borough_map) \n\nCombine ACT metrics and SAT metrics, drop repetitive regents metrics\n\n#ACT \ndf['Percent who took ACT'] = df[['Metric Value - % of cohort who took the ACT English exam','Metric Value - % of cohort who took the ACT Reading exam']].mean(axis=1)\ndf['Percent with 20+ ACT'] = df[['Metric Value - % of students in the current cohort who took the ACT English exam who scored 20+','Metric Value - % of students in the current cohort who took the ACT Math exam who scored 21+']].mean(axis=1)\ndf = df.drop(['Metric Value - % of cohort who took the ACT English exam','Metric Value - % of cohort who took the ACT Reading exam','Metric Value - % of students in the current cohort who took the ACT English exam who scored 20+','Metric Value - % of students in the current cohort who took the ACT Math exam who scored 21+'], axis=1)\n\n#SAT \ndf['Percent College Ready based on SAT Math'] = df['Metric Value - % of students in the current cohort who took the SAT Math exam who passed the college ready threshold']\ndf.drop(['Metric Value - % of students in the current cohort who took the SAT Math exam who passed the college ready threshold','Metric Value - % of students in the current cohort who took the SAT Reading and Writing exam and scored 480+'], axis=1, inplace=True )\n\n#Regents\ndf = df.drop(columns=['Metric Value - % of students who took the English Regents exam and are college ready (scored 70+)','Metric Value - % of students who took the English Regents exam in the current year who scored 65+'])\n\nCombine 8th grade proficiency metrics and drop lowest third citywide metrics since it is based on average grade 8 proficiency\n\ndf['Average Grade 8 Proficiency'] = df[['Average Grade 8 English Proficiency', 'Average Grade 8 Math Proficiency']].mean(axis=1)\ndf.drop(columns =['Average Grade 8 English Proficiency','Average Grade 8 Math Proficiency'],inplace=True)\n\ndf = df.loc[:, ~(df.columns.str.contains('Lowest Third'))]\n\nDrop students recommended for Individualized Education Programs (IEPs)\n\ndf.drop(['Percentage of students recommended for general ed settings with Special Ed Teacher Support Services (SETSS)','Percentage of students recommended for Integrated Co-Teaching (ICT) services','Percentage of students recommended for Special Class (SC) services'], axis=1, inplace=True)\n\nDrop columns that give same info as targets: graduation rate and chronic absenteeism\n\ndf.drop(columns=['High School Persistence Rate','% Attaining Regents Diploma','Metric Value - Percentage of Students with 90%+ Attendance','Average Student Attendance'], inplace=True)\n\nDrop columns that are captured in the College and Career Preparatory Course Index (CCPCI)1.\n\ndf = df.drop(columns=['Metric Value - % Scoring 3+ on any AP Exam',\n       'Metric Value - % Passing a NYCPS-certified CPCC Course',\n       'Metric Value - % Passing an Industry-Recognized Technical Assessment',\n       'Metric Value - % Scoring 4+ on any IB Exam',\n       \"Metric Value - % Earning a Grade of 'C' or Higher for College Credit\",\n       'Metric Value - % Scoring 65+ on Alg2, Chem, or Phys Regents Exam',\n       'Metric Value - % Earning a Diploma with an Arts Endorsement',\n       'Metric Value - % Earning a Diploma with a CTE Endorsement'], axis=1)\n\nMake course columns with low occurence into binary rather than percentages - more interperable. Drop the originals\n\ndf['CLEP_enrolled'] = (df['Percentage of Students Enrolled in a College Level Examination Program (CLEP)'] &gt; 0).astype(float)\ndf['IB_enrolled'] = (df['Percentage of Students Enrolled in an IB Course'] &gt; 0).astype(float)\ndf['NYCPS_college_prep_enrolled'] = (df['Percentage of Students Enrolled in an NYCPS-certified College Preparatory Course'] &gt; 0).astype(float)\ndf['Took_ACT'] = (df['Percent who took ACT'] &gt; 0).astype(float)\n\n\ndf = df.drop(columns=['Percentage of Students Enrolled in a College Level Examination Program (CLEP)',\n                    'Percentage of Students Enrolled in an IB Course',\n                    'Percentage of Students Enrolled in an NYCPS-certified College Preparatory Course',\n                    'Percent who took ACT', 'Percent with 20+ ACT'], axis=1)\n\nHandle Data Types: Converting all columns to numeric is essential for EDA to create plots and for modeling as regression will only take numeric values.\n\n#convert rate % to float\ndf['Student Survey Response Rate'] = pd.to_numeric(df['Student Survey Response Rate'].str[:-1])\ndf['Teacher Survey Response Rate'] = pd.to_numeric(df['Teacher Survey Response Rate'].str[:-1])\ndf['Parent Survey Response Rate'] = pd.to_numeric(df['Parent Survey Response Rate'].str[:-1])\n\n#convert all metric value columns to numeric\ndf.loc[:, df.columns.str.contains('Metric Value')] = df.loc[:, df.columns.str.contains('Metric Value')].apply(pd.to_numeric, errors='coerce')\n\nHandle Missingness: To address the large amounts of missing data, I first removed columns where more than 50% of the data was missing. This step significantly reduced the size of the dataframe by eliminating over 100 columns that wouldn’t have contributed meaningfully to the analysis due to the high volume of missing values. Next, I focused on imputing data for columns with less than 10% missing values. For imputation, I used the skew of the distribution to determine the best method. If the skew was greater than an absolute value of 0.5, I imputed using the median, as this is more robust to outliers. If the distribution appeared to be approximately normal, I used the mean for imputation. Finally, I removed the remaining three columns with missing data that did not meet the threshold, ensuring a complete and clean dataset for analysis.\n\nlen(df.columns)\n\n71\n\n\n\ndef handle_missingness(df, threshold=0.1):\n    \"\"\"Function for imputing data using skew when missingness &lt; 10% and removing columns with missingness &gt; 50% \"\"\"\n    for col in df.columns:\n        missingness = df[col].isna().sum() / len(df)\n        if missingness &lt; 0.1:\n            if df[col].dtype == 'float64':\n                col_skew = skew(df[col].dropna()) \n                if abs(col_skew) &gt; 0.5:\n                    df[col] = df[col].fillna(df[col].median())\n                else: \n                    df[col] = df[col].fillna(df[col].mean())\n            elif df[col].dtype == 'object':\n                df[col] = df[col].fillna(df[col].mode()[0]) \n        elif missingness &gt; 0.5:\n            df.drop([col], axis=1, inplace=True)\n    return df\n\n\ndf = handle_missingness(df)\n\nCheck null count of each column\n\ndf.isnull().sum().sort_values(ascending=False).head(20)\n\nMetric Value - Average score of students in the current cohort who took the SAT Reading and Writing exam    76\nMetric Value - Average score of students in the current cohort who took the SAT Math exam                   76\nStudent Achievement - Section Score                                                                         72\nMetric Value - Average Completion Rate for Remaining Regents                                                52\nPercent Male                                                                                                 0\nPercent Female                                                                                               0\nPercent English Language Learners                                                                            0\nPercent Students with IEPs                                                                                   0\nEconomic Need Index                                                                                          0\nPercent Neither Female nor Male                                                                              0\nDBN                                                                                                          0\nPercent in Temp Housing                                                                                      0\nPercent Overage / Undercredited                                                                              0\nPercent HRA Eligible                                                                                         0\nTeacher Percent - White                                                                                      0\nPercentage of Students Enrolled in an Advanced Math or Science Course                                        0\nPercentage of Students Enrolled in a College Credited Course                                                 0\nNearby Student Distance (mi)                                                                                 0\nPercentage of Students Enrolled in an AP Course                                                              0\nYears of principal experience at this school                                                                 0\ndtype: int64\n\n\nDrop remaining columns with missingness\n\ndf = df.drop(columns=['Metric Value - Average score of students in the current cohort who took the SAT Reading and Writing exam','Metric Value - Average score of students in the current cohort who took the SAT Math exam','Student Achievement - Section Score','Metric Value - Average Completion Rate for Remaining Regents'], axis=1)\n\nPost cleaning data check\n\ndf.dtypes.value_counts()\n\nfloat64    51\nobject      4\nName: count, dtype: int64\n\n\nThe remaining object columns are ‘DBN’, ‘School Name’, ‘District’, and ‘Borough’. We will keep these for EDA purposes.\n\ndf.select_dtypes(include=['object']).columns\n\nIndex(['DBN', 'School Name', 'District', 'Borough'], dtype='object')\n\n\nSave processed df to csv\n\ndf.to_csv('../../data/processed-data/processed_df.csv', index=None)\n\nPost cleaning df.head check\n\ndf.head()\n\n\n\n\n\n\n\n\nDBN\nSchool Name\nEnrollment\nPercent Female\nPercent Male\nPercent Neither Female nor Male\nPercent English Language Learners\nPercent Students with IEPs\nEconomic Need Index\nPercent Overage / Undercredited\n...\nStudent Percent - Black and Hispanic\nStudent Percent - Other\nDistrict\nBorough\nPercent College Ready based on SAT Math\nAverage Grade 8 Proficiency\nCLEP_enrolled\nIB_enrolled\nNYCPS_college_prep_enrolled\nTook_ACT\n\n\n\n\n0\n01M292\norchard collegiate academy\n269.0\n0.468\n0.532\n0.0\n0.063\n0.253\n0.866\n0.041\n...\n0.766\n0.227\n01\nManhattan\n0.136\n2.715\n0.0\n0.0\n0.0\n0.0\n\n\n1\n01M448\nuniversity neighborhood high school\n485.0\n0.487\n0.513\n0.0\n0.118\n0.210\n0.814\n0.029\n...\n0.625\n0.371\n01\nManhattan\n0.225\n3.050\n0.0\n0.0\n0.0\n0.0\n\n\n2\n01M450\neast side community school\n389.0\n0.458\n0.542\n0.0\n0.013\n0.290\n0.638\n0.003\n...\n0.684\n0.283\n01\nManhattan\n0.370\n3.010\n0.0\n0.0\n1.0\n1.0\n\n\n3\n01M539\nnew explorations into science, technology and ...\n620.0\n0.508\n0.492\n0.0\n0.005\n0.185\n0.429\n0.005\n...\n0.249\n0.705\n01\nManhattan\n0.834\n3.755\n0.0\n0.0\n0.0\n1.0\n\n\n4\n01M696\nbard high school early college\n565.0\n0.577\n0.423\n0.0\n0.005\n0.147\n0.482\n0.004\n...\n0.391\n0.570\n01\nManhattan\n0.762\n3.840\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n5 rows × 55 columns\n\n\n\nPost cleaning missingness check\n\nplt.figure(figsize=(10, 6))\nsns.heatmap(df.isnull(), cbar=True, cmap=\"viridis\")\nplt.title(\"Missing Values After Cleaning\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nConclusion\nI chose to work with this specific subset of the data because it retains the key columns I am most interested in exploring. I decided to drop columns that were too granular, such as specific course types and certain demographic data, which were not directly relevant to my analysis. Additionally, some metric columns contained redundant information already captured by the ‘Metric Value’ columns I kept. As a result, I removed the N count, metric rating, metric score, comparison group, and positive response columns. After this filtering process, the dataset is now streamlined, containing 55 columns and 501 rows, ensuring a more focused and manageable analysis.\n\n\n\n\n\nReferences\n\n1. NYC Public Schools InfoHub (2024)."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is a crucial beginning step in any data science project. On this page, we will examine distributions, analyze correlations, and uncover relationships between variables, ensuring the data is ready for the modeling phase. These insights will guide and shape our modeling approach. It is important to avoid modeling blindly and instead base our decisions on a thorough understanding of the data.\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport textwrap\nimport plotly.express as px\nimport scipy.stats as stats\nfrom scipy.stats import ttest_ind, skew, kurtosis, f_oneway\nCode\ndf = pd.read_csv('../../data/processed-data/processed_df.csv')"
  },
  {
    "objectID": "technical-details/eda/main.html#univariate-analysis",
    "href": "technical-details/eda/main.html#univariate-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\nSummary Statistics\n\n\nCode\ndf.describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nEnrollment\n501.0\n568.892216\n683.763617\n11.0000\n287.000\n388.000000\n497.0000\n5942.000\n\n\nPercent Female\n501.0\n0.488070\n0.143794\n0.0000\n0.430\n0.486000\n0.5400\n1.000\n\n\nPercent Male\n501.0\n0.511669\n0.143896\n0.0000\n0.460\n0.514000\n0.5700\n1.000\n\n\nPercent Neither Female nor Male\n501.0\n0.000257\n0.000948\n0.0000\n0.000\n0.000000\n0.0000\n0.009\n\n\nPercent English Language Learners\n501.0\n0.132874\n0.184345\n0.0000\n0.039\n0.082000\n0.1430\n0.994\n\n\nPercent Students with IEPs\n501.0\n0.209673\n0.080834\n0.0000\n0.165\n0.208000\n0.2630\n0.629\n\n\nEconomic Need Index\n501.0\n0.730691\n0.237780\n0.0000\n0.690\n0.813000\n0.8810\n0.949\n\n\nPercent Overage / Undercredited\n501.0\n0.054353\n0.054019\n0.0000\n0.018\n0.041000\n0.0690\n0.308\n\n\nPercent in Temp Housing\n501.0\n0.130513\n0.079141\n0.0060\n0.081\n0.119000\n0.1650\n0.560\n\n\nPercent HRA Eligible\n501.0\n0.700184\n0.153327\n0.0000\n0.642\n0.737000\n0.8120\n0.947\n\n\nTeacher Percent - White\n501.0\n0.430554\n0.161836\n0.0000\n0.320\n0.430554\n0.5350\n0.855\n\n\nNearby Student Distance (mi)\n501.0\n2.317565\n1.778227\n0.4000\n1.100\n1.700000\n2.9000\n9.500\n\n\nPercentage of Students Enrolled in an AP Course\n501.0\n0.291362\n0.200515\n0.0000\n0.171\n0.255500\n0.3840\n0.991\n\n\nPercentage of Students Enrolled in an Advanced Math or Science Course\n501.0\n0.379705\n0.144616\n0.0000\n0.282\n0.379705\n0.4840\n0.800\n\n\nPercentage of Students Enrolled in a College Credited Course\n501.0\n0.081681\n0.137217\n0.0000\n0.000\n0.021000\n0.1150\n0.923\n\n\nPercentage of Students Enrolled in Any Advanced Course\n501.0\n0.571937\n0.175886\n0.0100\n0.456\n0.571937\n0.6800\n0.998\n\n\nYears of principal experience at this school\n501.0\n6.889421\n5.110596\n0.0000\n2.200\n6.100000\n10.2000\n22.100\n\n\nPercent of teachers with 3 or more years of experience\n501.0\n0.795406\n0.133245\n0.0800\n0.737\n0.821500\n0.8820\n1.000\n\n\nPercent of Students Chronically Absent\n501.0\n0.417344\n0.161082\n0.0000\n0.299\n0.417344\n0.5320\n0.998\n\n\nTeacher Attendance Rate\n501.0\n0.952104\n0.027447\n0.6500\n0.950\n0.957000\n0.9630\n0.991\n\n\nMetric Value - Average Regents Score - English\n501.0\n69.576398\n9.322389\n29.1000\n63.900\n69.576398\n75.0000\n95.600\n\n\nMetric Value - College and Career Preparatory Course Index\n501.0\n0.656958\n0.239731\n0.0070\n0.487\n0.667000\n0.8750\n1.000\n\n\nMetric Value - College Persistence\n501.0\n0.485317\n0.186830\n0.0230\n0.349\n0.481000\n0.6100\n0.978\n\n\nRigorous Instruction - Element Score\n501.0\n3.473640\n0.651435\n1.8000\n3.030\n3.473640\n3.8900\n4.970\n\n\nCollaborative Teachers - Element Score\n501.0\n3.612056\n0.583369\n1.5200\n3.270\n3.612056\n3.9800\n4.930\n\n\nSupportive Environment - Element Score\n501.0\n3.577099\n0.417602\n2.0200\n3.320\n3.577099\n3.8600\n4.650\n\n\nEffective School Leadership - Element Score\n501.0\n3.552355\n0.570646\n1.2800\n3.270\n3.560000\n3.8900\n4.860\n\n\nStrong Family-Community Ties - Element Score\n501.0\n3.442936\n0.612733\n1.3800\n3.100\n3.442936\n3.8400\n4.830\n\n\nTrust - Element Score\n501.0\n3.392435\n0.600361\n1.1100\n3.050\n3.450000\n3.7900\n4.930\n\n\nStudent Survey Response Rate\n501.0\n66.850299\n22.891561\n0.0000\n58.000\n71.000000\n83.0000\n100.000\n\n\nTeacher Survey Response Rate\n501.0\n80.932136\n17.471674\n18.0000\n72.000\n84.000000\n95.0000\n100.000\n\n\nParent Survey Response Rate\n501.0\n34.069860\n23.083958\n0.0000\n16.000\n29.000000\n48.0000\n96.000\n\n\nzip code\n501.0\n10715.345309\n535.154739\n10002.0000\n10302.000\n10468.000000\n11217.0000\n11694.000\n\n\nPercent No High School (25+)\n501.0\n9.221000\n5.305701\n0.0000\n5.200\n8.400000\n13.0000\n25.900\n\n\nPercent Bachelor's Degree or Higher (25+)\n501.0\n39.461876\n22.259906\n11.6000\n21.600\n33.900000\n51.8000\n94.500\n\n\nPercent Language Other Than English at Home\n501.0\n46.561000\n18.548678\n14.0000\n31.100\n50.000000\n60.2000\n86.300\n\n\nPercent Population with Disabilities\n501.0\n12.208800\n4.074472\n1.6000\n9.000\n11.700000\n15.1000\n21.300\n\n\nPercent Foreign-Born Population\n501.0\n33.935329\n11.466693\n13.7000\n25.400\n31.600000\n40.3000\n70.900\n\n\nPercent Households with Broadband Internet\n501.0\n86.407600\n5.493089\n71.1000\n82.400\n87.500000\n89.6000\n99.400\n\n\nMedian Household Income\n501.0\n78499.157685\n41488.175277\n26400.0000\n43985.000\n67489.000000\n100190.0000\n250001.000\n\n\nPercent Households on SNAP/Food Stamps\n501.0\n23.808200\n14.262966\n0.0000\n11.700\n21.600000\n34.0000\n52.000\n\n\ndropout_rate\n501.0\n5.358034\n5.261403\n0.0000\n1.225\n3.700000\n7.7000\n27.650\n\n\nGraduation Rate\n501.0\n0.899607\n0.072814\n0.5795\n0.853\n0.908750\n0.9595\n1.000\n\n\nStudent Percent - Black and Hispanic\n501.0\n0.799110\n0.213992\n0.0450\n0.729\n0.891000\n0.9490\n1.000\n\n\nStudent Percent - Other\n501.0\n0.183108\n0.206603\n0.0000\n0.041\n0.094000\n0.2520\n0.947\n\n\nDistrict\n501.0\n25.339321\n27.018524\n1.0000\n8.000\n16.000000\n28.0000\n84.000\n\n\nPercent College Ready based on SAT Math\n501.0\n0.152992\n0.213768\n0.0000\n0.023\n0.075000\n0.1900\n1.000\n\n\nAverage Grade 8 Proficiency\n501.0\n2.753443\n0.436318\n1.7550\n2.475\n2.680000\n2.9550\n4.270\n\n\nCLEP_enrolled\n501.0\n0.017964\n0.132954\n0.0000\n0.000\n0.000000\n0.0000\n1.000\n\n\nIB_enrolled\n501.0\n0.039920\n0.195967\n0.0000\n0.000\n0.000000\n0.0000\n1.000\n\n\nNYCPS_college_prep_enrolled\n501.0\n0.083832\n0.277413\n0.0000\n0.000\n0.000000\n0.0000\n1.000\n\n\nTook_ACT\n501.0\n0.223553\n0.417042\n0.0000\n0.000\n0.000000\n0.0000\n1.000\n\n\n\n\n\n\n\n\nTransformations\nView distribution of numerical features with histogram\n\n\nCode\nnumerical_columns = df.select_dtypes(include=['float64']).columns\n\naxes = df[numerical_columns].hist(figsize=(30, 30))\nplt.tight_layout()\n\n# wrap the titles for readability\nfor ax, col in zip(axes.flatten(), numerical_columns):\n    wrapped_title = textwrap.fill(col, width=30)\n    ax.set_title(wrapped_title)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nBased on these distributions, we may want to transform some variables before modeling. Lets check the actual skews:\n\n\nCode\nskewness = df.apply(lambda col: skew(col) if col.dtype != 'object' else None)\n\n\n\n\nCode\nskewness.loc[abs(skewness) &gt; 2]\n\n\nEnrollment                                                      3.738744\nPercent Neither Female nor Male                                 5.155183\nPercent English Language Learners                               3.151075\nPercent Overage / Undercredited                                 2.173497\nPercentage of Students Enrolled in a College Credited Course    2.898833\nTeacher Attendance Rate                                        -5.282616\nPercent College Ready based on SAT Math                         2.235895\nCLEP_enrolled                                                   7.258441\nIB_enrolled                                                     4.700168\nNYCPS_college_prep_enrolled                                     3.003344\ndtype: float64\n\n\n\n\nCode\ndf_transformed = df.copy()\n\n#log variables exhibiting the power law\ndf_transformed['log_ELL'] = np.log(df_transformed['Percent English Language Learners'] + 1)\ndf_transformed['log_dropout_rate'] = np.log(df_transformed['dropout_rate'] + 1)\ndf_transformed['log_temp_housing'] = np.log(df_transformed['Percent in Temp Housing'] + 1)\ndf_transformed['log_percent_overage'] = np.log(df_transformed['Percent Overage / Undercredited'] + 1)\ndf_transformed['log_enrollment'] = np.log(df_transformed['Enrollment'] + 1)\ndf_transformed['log_nearby_student_distance'] = np.log(df_transformed['Nearby Student Distance (mi)'] + 1)\ndf_transformed['log_college_ready_SAT_math'] = np.log(df_transformed['Percent College Ready based on SAT Math'] + 1)\ndf_transformed['log_student_percent_other'] = np.log(df_transformed['Student Percent - Other'] + 1)\ndf_transformed['log_household_income'] = np.log(df_transformed['Median Household Income'] + 1)\n\n\n\n\nCode\nnumerical_columns = df_transformed.select_dtypes(include=['float64']).columns\n\naxes = df_transformed[numerical_columns].hist(figsize=(30, 30))\nplt.tight_layout()\n\n# wrap the titles for readability\nfor ax, col in zip(axes.flatten(), numerical_columns):\n    wrapped_title = textwrap.fill(col, width=30)\n    ax.set_title(wrapped_title)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nMultivariate Analysis\nVisualize correlations with heatmap of all numeric columns\n\n\nCode\nnumeric_columns = df.select_dtypes(include=['float64'])\ncorr = numeric_columns.corr()\nf, ax = plt.subplots(figsize=(20, 20))  \nsns.heatmap(corr,  cmap='vlag', vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()"
  },
  {
    "objectID": "technical-details/eda/main.html#statistical-testing",
    "href": "technical-details/eda/main.html#statistical-testing",
    "title": "Exploratory Data Analysis",
    "section": "Statistical Testing",
    "text": "Statistical Testing\nLets try to uncover relationships between some of our target variables and features in the data\n\nT-test\nI will use a t test to evaluate if there is difference in the means of two groups\n\nDropout Rate vs Median Household Income\nFirst, I want to see if there is a relationship between median household income and dropout rate. Looking at the graph, it appears that high median income is associated with low dropout rates. I will do a t-test to validate this assumption.\n\n\nCode\nsns.regplot(data=df, y='dropout_rate', x='Median Household Income', scatter_kws={'edgecolor': 'black'}, color='green')\nplt.title('Median Household Income vs Dropout Rate')\nplt.ylabel('Dropout Rate')\nplt.xlabel('Median Household Income')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nhigh_dropout = df[df['dropout_rate'] &gt; 7.7]['Median Household Income']\nlow_dropout = df[df['dropout_rate'] &lt;= 7.7]['Median Household Income']\n\nt_stat, p_value = ttest_ind(high_dropout, low_dropout)\nprint(f't-value = {t_stat:.2f}, p-value = {p_value:.3f}')\n\n\nt-value = -2.79, p-value = 0.006\n\n\nNull Hypothesis: There is no difference in mean median household income between schools with high dropout and low dropout rates.\nAlternative Hypothesis: There is a difference in mean median household income between schools with high and low droppout rates\nThe p-value is 0.006, allowing us to reject the null hypothesis and conclude that there is significant difference in the mean median household income of schools with high dropout rates vs low dropout rates.\n\n\nChronic Absenteeism vs Supportive School Environment\nNow I want to check the relationship between chronic absenteeism and a supportive school environment.\n\n\nCode\nsns.regplot(data=df, x='Percent of Students Chronically Absent', y='Supportive Environment - Element Score', scatter_kws={'edgecolor': 'black'}, color='purple')\nplt.title('Chronic Absenteeism vs School Support')\nplt.xlabel('Percent of Students Chronically Absent')\nplt.ylabel('Supportive Environment Score')\nplt.show()\n\n\n\n\n\n\n\n\n\nIt appears that lower supportive environment scores is associated with high rates of chronic absenteeism. We can do a t-test to validate the assumption that there is a difference in supportive score for schools with high and low chronic absenteeism\n\n\nCode\nhigh_chronic_absent = df[df['Percent of Students Chronically Absent'] &gt; .4]['Supportive Environment - Element Score']\nlow_chronic_absent = df[df['Percent of Students Chronically Absent'] &lt;= .4]['Supportive Environment - Element Score']\n\nt_stat, p_value = ttest_ind(high_chronic_absent, low_chronic_absent)\nprint(f't-value = {t_stat:.2f}, p-value = {p_value:.3f}')\n\n\nt-value = -11.44, p-value = 0.000\n\n\nNull Hypothesis: There is no difference in mean supportive environment score between schools with high chronic absenteeism and low chronic absenteeism.\nAlternative Hypothesis: There is a difference in mean supportive environment score between schools with high chronic absenteeism and low chronic absenteeism.\nThe p-value is less than 0.05, allowing us to reject the null hypothesis and conclude that there is significant difference in the mean supportive environment element score of schools with high chronic absenteeism vs low chronic absenteeism.\n\n\n\nANOVA\nAn ANOVA test is used to evaluate if there is a difference in the means of more than two groups\n\n\nEconomic Need Index vs College Persistence Rates\nThe Economic Need Index estimates the percentage of the students at the school facing economic hardship.1 As shown by the box plot, it looks like low rates of college persistence is associated with high Economic Need Index. I will use an ANOVA test to see if there is a statistical difference in Economic Need Index between these three groups of persistence rates.\n\n\nCode\ndf['college_persistence_ranking'] = pd.cut(df['Metric Value - College Persistence'], bins=3, labels=['Low','Medium','High'])\n\n\n\n\nCode\nsns.boxplot(data=df, x='college_persistence_ranking', y='Economic Need Index',color='lightblue')\nplt.title('Economic Need Index vs College Persistence Rates')\nplt.xlabel('College Persistence Rates')\nplt.ylabel('Economic Need Index')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nanova_result = f_oneway(\n    df[df['college_persistence_ranking']=='Low']['Economic Need Index'],\n    df[df['college_persistence_ranking']=='Medium']['Economic Need Index'],\n    df[df['college_persistence_ranking']=='High']['Economic Need Index']\n)\nprint(f'F value = {anova_result.statistic:.2f}, p-value = {anova_result.pvalue:.3f}')\n\n\nF value = 21.57, p-value = 0.000\n\n\nNull Hypothesis: There is no difference in mean economic need index between schools with high, medium, and low college persistence rates.\nAlternative Hypothesis: There is a difference in mean economic need index between schools with high, medium, and low college persistence rates.\nThe p-value less than 0.05, allowing us to reject the null hypothesis and conclude that there is significant difference in the mean economic need index between at least one of the college persistence ranking groups (High, Medium, Low)."
  },
  {
    "objectID": "technical-details/eda/main.html#additional-visualizations",
    "href": "technical-details/eda/main.html#additional-visualizations",
    "title": "Exploratory Data Analysis",
    "section": "Additional Visualizations",
    "text": "Additional Visualizations\nTaking a look at a box plot of dropout rates by borough, it is apparent that the Bronx has the highest rates of dropout. In addition, the median for Queens is low but there are some high outliers.\n\n\nCode\ncolors = ['#1E90FF','#FF6347','#FFD700','#D2B48C', '#98FB98']\nsns.boxplot(x='Borough', y='dropout_rate', data=df, palette=colors)\nplt.title('Dropout Rate by Borough')\nplt.ylabel('Dropout Rate')\nplt.show()\n\n\n/var/folders/hl/7yl66vtn2mn30d649bqpv1wc0000gn/T/ipykernel_20761/3680735094.py:2: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(x='Borough', y='dropout_rate', data=df, palette=colors)\n\n\n\n\n\n\n\n\n\nLooking at chronic absenteeism and dropout rates by district, certain districts clearly have higher medians and IQRs, especially some high spread and outliers in district 84 for chronic abseentism, and a high drop out rate in district 12.\n\n\nCode\nplt.figure(figsize=(14, 6))\nsns.boxplot(data=df, x='District', y='Percent of Students Chronically Absent')\nplt.title('Percent of Chronic Absenteeism by District')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(14, 6))\nsns.boxplot(data=df, x='District', y='dropout_rate')\nplt.title('Dropout Rate by District')\nplt.show()\n\n\n\n\n\n\n\n\n\nIt appears that high college persistence is associated with high average grade 8 proficiency. This emphasizes the importance of early intervention.\n\n\nCode\nplt.scatter(data=df, x='Metric Value - College Persistence', y='Average Grade 8 Proficiency', edgecolors='black')\nplt.title('College Persistence vs Average Grade 8 Proficiency')\nplt.xlabel('College Persistence')\nplt.ylabel('Average Grade 8 Proficiency')\nplt.show()\n\n\n\n\n\n\n\n\n\nLets take a look at how all three of our target metrics work together in this interactive plot\n\n\nCode\nfig = px.scatter(df, x='Percent of Students Chronically Absent', y='dropout_rate', color='Metric Value - College Persistence')\nfig.update_layout(title='Percent of Student Chronically Absent vs Dropout Rate', xaxis_title='Percent of Students Chronically Absent', yaxis_title='Dropout Rate')\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nIt looks like high rates of college persistence are associated with low dropout rates and low percentages of student chronically absent - a motivating example of why we need to intervene to reduce these two metrics in order to maximize college persistence!"
  },
  {
    "objectID": "technical-details/eda/main.html#feature-selection",
    "href": "technical-details/eda/main.html#feature-selection",
    "title": "Exploratory Data Analysis",
    "section": "Feature Selection",
    "text": "Feature Selection\nFor further analysis, we will do some feature selection to help us prepare for modeling\nLets check what columns are most correlated with our regression target\n\n\nCode\ntarget_column = 'Percent of Students Chronically Absent'\n\ncorrelations = numeric_columns.corr()[target_column].sort_values(ascending=False)\n\npositive_correlations = correlations[correlations &gt; 0]\nnegative_correlations = correlations[correlations &lt; 0]\n\nprint(positive_correlations.head(10).sort_values(ascending=False))\nprint(negative_correlations.sort_values(ascending=True).head(10)) \n\n\nPercent of Students Chronically Absent    1.000000\nPercent HRA Eligible                      0.476284\nStudent Percent - Black and Hispanic      0.434400\nPercent Students with IEPs                0.383127\ndropout_rate                              0.379707\nPercent in Temp Housing                   0.308445\nPercent Overage / Undercredited           0.308356\nEconomic Need Index                       0.215888\nPercent English Language Learners         0.110565\nTeacher Survey Response Rate              0.102258\nName: Percent of Students Chronically Absent, dtype: float64\nAverage Grade 8 Proficiency                                  -0.558927\nPercent College Ready based on SAT Math                      -0.554737\nMetric Value - College Persistence                           -0.526158\nSupportive Environment - Element Score                       -0.507653\nGraduation Rate                                              -0.477225\nStudent Percent - Other                                      -0.441859\nMetric Value - Average Regents Score - English               -0.422453\nMetric Value - College and Career Preparatory Course Index   -0.418735\nTook_ACT                                                     -0.347957\nRigorous Instruction - Element Score                         -0.324224\nName: Percent of Students Chronically Absent, dtype: float64\n\n\n\n\nCode\nfeatures = df[[ \"Percent of Students Chronically Absent\",\n    \"Percent HRA Eligible\",\n    \"Student Percent - Black and Hispanic\",\n    \"Percent Students with IEPs\",\n    \"dropout_rate\",\n    \"Percent in Temp Housing\",\n    \"Percent Overage / Undercredited\",\n    \"Economic Need Index\",\n    \"Percent English Language Learners\",\n    \"Teacher Survey Response Rate\",\n    \"Average Grade 8 Proficiency\",\n    \"Percent College Ready based on SAT Math\",\n    \"Metric Value - College Persistence\",\n    \"Supportive Environment - Element Score\",\n    \"Graduation Rate\",\n    \"Metric Value - Average Regents Score - English\",\n    \"Metric Value - College and Career Preparatory Course Index\",\n    \"Took_ACT\"]]\n\n\nWe can visualize these correlations using pairplot, we want to make sure there are no zero looking correlations, a big blob of dots with no apparent relationship\n\n\nCode\nsns.pairplot(features)\n\n\n\n\n\n\n\n\n\nNow we should check the pairwise correlations to ensure that we dont have variables that are too correlated to each other\n\n\nCode\ncorr = features.corr()\nf, ax = plt.subplots(figsize=(20, 20))  \nsns.heatmap(corr,  cmap='vlag', annot=True, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()\n\n\n\n\n\n\n\n\n\nThere appears to be relationships with our target and all the features we have included, plus not too high correlations (nothing greater than |0.85|) between the features. Lets export this dataframe for our regression models.\n\n\nCode\nfeatures.to_csv('../../data/processed-data/df_subset_regression.csv', index=False)"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\nfrom sklearn.metrics import silhouette_score\n\n\n\n\n\n\n\nPrincipal Component Analysis is an unsupervised learning technique used for dimensionality reduction. It uses linear transformations to find the most important features, or principal components. It is a great technique to use if you have many dimensions and need a way to visualize them in 2 dimensions. It is a good preprocessing technique to do before supervised learning. The goal is to capture as much of the variance contained in the features but in reduced dimensions. N_components specifies how many prinicipal components we want.\nWe are going to attempt to apply PCA on our socioeconomic features\n\ndf = pd.read_csv('../../data/processed-data/processed_df.csv', index_col=None)\n\n\nsubset = df[[ \"Percent HRA Eligible\",\n    \"Percent in Temp Housing\",\n    \"Percent Overage / Undercredited\",\n    \"Economic Need Index\",\n    'Percent No High School (25+)',\n    \"Percent Bachelor's Degree or Higher (25+)\",\n    'Percent Language Other Than English at Home',\n    'Percent Population with Disabilities',\n    'Percent Foreign-Born Population',\n    'Percent Households with Broadband Internet',\n    'Median Household Income',\n    'Percent Households on SNAP/Food Stamps']]\n\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(subset)\n\n\npca = PCA(n_components=2) \nX_pca = pca.fit_transform(X_scaled)\n\n#visualize the first two components\nplt.scatter(X_pca[:,0], X_pca[:,1], alpha=0.5)\nplt.xlabel('PC-1')\nplt.ylabel('PC-2')\nplt.title('Principal Component Analysis')\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndef plot_variance_explained(pca):\n    print(\"Variance explained by each principal component: \")\n    print(pca.explained_variance_ratio_[0:10])\n    print(\"Cumulative variance explained by each principal component: \")\n    print(np.cumsum(pca.explained_variance_ratio_)[0:10])\n    plt.plot(pca.explained_variance_ratio_, marker='o')\n    plt.xlabel(\"number of components\")\n    plt.ylabel(\"explained variance ratio\")\n    plt.show()\n    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n    plt.xlabel(\"number of components\")\n    plt.ylabel(\"cumulative explained variance\")\n    plt.show()\n\nIt looks like 2 is the best number of principal components\n\nplot_variance_explained(pca)\n\nVariance explained by each principal component: \n[0.46812599 0.14041224 0.12717426 0.09999827]\nCumulative variance explained by each principal component: \n[0.46812599 0.60853822 0.73571248 0.83571075]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt-SNE is an unsupervised learning technique used to for dimensionality reduction. It is a better alternative for non-linear data. Perplexity is the most important hyperparameter in t_SNE. This paramter controls the balance between local and global structure. A high preplexity preseves global structure while a lower perplexity emphasizes local structure.\n\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X_scaled)\n\n\nfig, ax = plt.subplots()\nax.scatter(X_tsne[:,0], X_tsne[:,1], alpha=0.5) \nax.set(xlabel='tSNE-1 ', ylabel='tSNE-2',\ntitle='tSNE results')\nax.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nKmeans is another unsupervised learning technique used for clustering. We use the elbow method to find the best number of clusters. We want to choose the number of clusters at the elbow of the graph becasue this indicates that the inertia reduction is decreasing, and there is diminishing returns in adding more clusters. The goal of clustering is to maximize the intercluster distance (distance between clusters - seapartion) and minimize intracluster distance (distance from points to center of cluster - cohesion). The goal is to minimize the variance within each cluster. We can measure how good our clustering is using the silhouette score.\n\ninertia = []\nk_values = range(1, 30)\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_pca)\n    inertia.append(kmeans.inertia_)\n\nplt.plot(k_values, inertia, marker='o')\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Method to Choose K\")\nplt.xticks(k_values)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that the optimal number of clusters is about 4.\n\n# choose k\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\necon_clusters = kmeans.fit_predict(X_pca)\n\nfor cluster in range(n_clusters):\n    plt.scatter(X_pca[econ_clusters == cluster, 0], X_pca[econ_clusters == cluster, 1],\n        label=f\"Cluster {cluster + 1}\", s=50, alpha=0.6)\n\nplt.xlabel(\"tSNE 1\")\nplt.ylabel(\"tSNE 2\")\nplt.title(\"K-means Clustering of Socioeconomic Factors\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster_labels = kmeans.labels_\n\n\n\nsilhouette_avg = silhouette_score(X_scaled, cluster_labels)\nprint(f'Silhouette Score: {silhouette_avg}')\n\nSilhouette Score: 0.16605649350908025\n\n\nConclusion: there is not large separation between the clusters, indicating that we may need to use an alternate dimensionality reduction technique\n\n\n\n\nDBSCAN is an unsupervised learning clustering algorithm that is better suited for irregular data. It improves upon kmeans because it is able to find non linear clusters. It is also able to identify noise.\n\ndbscan = DBSCAN(eps=0.5, min_samples=3)\ndbscan.fit(subset)\ndf['DBSCAN_Cluster'] = dbscan.labels_\n\n\nnoise_points = df[df['DBSCAN_Cluster'] == -1]\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(subset[:, 0], subset[:, 1], c=dbscan.labels_, cmap='rainbow', s=20)\nplt.title(\"DBSCAN Clustering\")\nplt.xlabel(\"Feature 1 (Standardized)\")\nplt.ylabel(\"Feature 2 (Standardized)\")\nplt.colorbar(label=\"Cluster Label\")\nplt.show()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nFile /opt/anaconda3/envs/dsan5400/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:173, in pandas._libs.index.IndexEngine.get_loc()\n\nTypeError: '(slice(None, None, None), 0)' is an invalid key\n\nDuring handling of the above exception, another exception occurred:\n\nInvalidIndexError                         Traceback (most recent call last)\nCell In[60], line 2\n      1 plt.figure(figsize=(10, 6))\n----&gt; 2 plt.scatter(subset[:, 0], subset[:, 1], c=dbscan.labels_, cmap='rainbow', s=20)\n      3 plt.title(\"DBSCAN Clustering\")\n      4 plt.xlabel(\"Feature 1 (Standardized)\")\n\nFile /opt/anaconda3/envs/dsan5400/lib/python3.11/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile /opt/anaconda3/envs/dsan5400/lib/python3.11/site-packages/pandas/core/indexes/base.py:3817, in Index.get_loc(self, key)\n   3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n-&gt; 3817     self._check_indexing_error(key)\n   3818     raise\n\nFile /opt/anaconda3/envs/dsan5400/lib/python3.11/site-packages/pandas/core/indexes/base.py:6059, in Index._check_indexing_error(self, key)\n   6055 def _check_indexing_error(self, key):\n   6056     if not is_scalar(key):\n   6057         # if key is not a scalar, directly raise an error (the code below\n   6058         # would convert to numpy arrays and raise later any way) - GH29926\n-&gt; 6059         raise InvalidIndexError(key)\n\nInvalidIndexError: (slice(None, None, None), 0)\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\nneighbors = NearestNeighbors(n_neighbors=5)\nneighbors_fit = neighbors.fit(X_scaled)\ndistances, indices = neighbors_fit.kneighbors(X_scaled)\n\n# Sort distances for plotting\ndistances = np.sort(distances[:, 4], axis=0)\nplt.figure(figsize=(10, 6))\nplt.plot(distances)\nplt.title(\"k-Distance Plot\")\nplt.xlabel(\"Data Points Sorted\")\nplt.ylabel(\"4th Nearest Neighbor Distance\")\nplt.show()\n\n\n\n\n\n\n\n\n\neps_range = np.arange(0.1, 1.1, 0.1)\nmin_samples_values = range(2, 20)\n\nresults = []\n\nfor eps in eps_range:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(X_scaled)\n        if len(set(labels)) &gt; 1: \n            score = silhouette_score(X_scaled, labels)\n            results.append((eps, min_samples, score))\n\n\nresults_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'silhouette_score'])\nbest_params = results_df.loc[results_df['silhouette_score'].idxmax()]\nbest_min_samples = best_params['min_samples']\nbest_eps = best_params['eps']\nprint(f\"Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n\nBest eps: 1.0, Best min_samples: 13.0\n\n\n\nclustering = DBSCAN(eps=best_eps, min_samples=int(best_min_samples))\nclustering_labels = clustering.fit_predict(X_scaled)\n\n\nplot = plt.scatter(df['bill_depth_mm'], df['flipper_length_mm'],c=clustering_labels)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title(\"DBSCAN Clustering\")\nunique_labels = np.unique(clustering_labels)\nlegend_labels = [\"Noise\" if label == -1 else f\"Cluster {label + 1}\" for label in unique_labels]\nhandles, _ = scatter.legend_elements()\nplt.legend(handles, legend_labels, title=\"Clusters\")\n\n\nfrom sklearn.decomposition import PCA\n\n# Apply PCA to reduce to 2D\npca = PCA(n_components=2)\ndf_pca = pca.fit_transform(df_courses)\n\n# Plot the 2D clustering\nplt.figure(figsize=(10, 6))\nplt.scatter(df_pca[:, 0], df_pca[:, 1], c=clustering_labels, cmap='viridis')\n\n# Adding labels and title\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.title(\"DBSCAN Clustering (PCA Reduced Dimensions)\")\n\n# Add legend for clusters\nunique_labels = np.unique(clustering_labels)\nlegend_labels = [\"Noise\" if label == -1 else f\"Cluster {label + 1}\" for label in unique_labels]\nhandles, _ = plt.gca().get_legend_handles_labels()\nplt.legend(handles, legend_labels, title=\"Clusters\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\n\n# Example: Generate synthetic data\nX, y = make_blobs(n_samples=300, centers=4, random_state=42)\n\n# Perform Agglomerative Clustering\nagg_clust = AgglomerativeClustering(n_clusters=4, linkage='ward')\ny_pred = agg_clust.fit_predict(X_scaled)\n\n# Plot the result\nplt.figure(figsize=(8, 6))\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_pred, cmap='viridis')\nplt.title('Agglomerative Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\n# Perform hierarchical/agglomerative clustering\nZ = linkage(subset, 'ward')\n\n# Create a dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(Z)\nplt.title('Dendrogram for Agglomerative Clustering')\nplt.xlabel('Data points')\nplt.ylabel('Distance')\nplt.show()"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations.\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\nfrom sklearn.metrics import silhouette_score"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#part-1-dimensionality-reduction-1",
    "href": "technical-details/unsupervised-learning/main.html#part-1-dimensionality-reduction-1",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Principal Component Analysis is an unsupervised learning technique used for dimensionality reduction. It uses linear transformations to find the most important features, or principal components. It is a great technique to use if you have many dimensions and need a way to visualize them in 2 dimensions. It is a good preprocessing technique to do before supervised learning. The goal is to capture as much of the variance contained in the features but in reduced dimensions. N_components specifies how many prinicipal components we want.\nWe are going to attempt to apply PCA on our socioeconomic features\n\ndf = pd.read_csv('../../data/processed-data/processed_df.csv', index_col=None)\n\n\nsubset = df[[ \"Percent HRA Eligible\",\n    \"Percent in Temp Housing\",\n    \"Percent Overage / Undercredited\",\n    \"Economic Need Index\",\n    'Percent No High School (25+)',\n    \"Percent Bachelor's Degree or Higher (25+)\",\n    'Percent Language Other Than English at Home',\n    'Percent Population with Disabilities',\n    'Percent Foreign-Born Population',\n    'Percent Households with Broadband Internet',\n    'Median Household Income',\n    'Percent Households on SNAP/Food Stamps']]\n\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(subset)\n\n\npca = PCA(n_components=2) \nX_pca = pca.fit_transform(X_scaled)\n\n#visualize the first two components\nplt.scatter(X_pca[:,0], X_pca[:,1], alpha=0.5)\nplt.xlabel('PC-1')\nplt.ylabel('PC-2')\nplt.title('Principal Component Analysis')\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndef plot_variance_explained(pca):\n    print(\"Variance explained by each principal component: \")\n    print(pca.explained_variance_ratio_[0:10])\n    print(\"Cumulative variance explained by each principal component: \")\n    print(np.cumsum(pca.explained_variance_ratio_)[0:10])\n    plt.plot(pca.explained_variance_ratio_, marker='o')\n    plt.xlabel(\"number of components\")\n    plt.ylabel(\"explained variance ratio\")\n    plt.show()\n    plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n    plt.xlabel(\"number of components\")\n    plt.ylabel(\"cumulative explained variance\")\n    plt.show()\n\nIt looks like 2 is the best number of principal components\n\nplot_variance_explained(pca)\n\nVariance explained by each principal component: \n[0.46812599 0.14041224 0.12717426 0.09999827]\nCumulative variance explained by each principal component: \n[0.46812599 0.60853822 0.73571248 0.83571075]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt-SNE is an unsupervised learning technique used to for dimensionality reduction. It is a better alternative for non-linear data. Perplexity is the most important hyperparameter in t_SNE. This paramter controls the balance between local and global structure. A high preplexity preseves global structure while a lower perplexity emphasizes local structure.\n\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X_scaled)\n\n\nfig, ax = plt.subplots()\nax.scatter(X_tsne[:,0], X_tsne[:,1], alpha=0.5) \nax.set(xlabel='tSNE-1 ', ylabel='tSNE-2',\ntitle='tSNE results')\nax.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nKmeans is another unsupervised learning technique used for clustering. We use the elbow method to find the best number of clusters. We want to choose the number of clusters at the elbow of the graph becasue this indicates that the inertia reduction is decreasing, and there is diminishing returns in adding more clusters. The goal of clustering is to maximize the intercluster distance (distance between clusters - seapartion) and minimize intracluster distance (distance from points to center of cluster - cohesion). The goal is to minimize the variance within each cluster. We can measure how good our clustering is using the silhouette score.\n\ninertia = []\nk_values = range(1, 30)\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    kmeans.fit(X_pca)\n    inertia.append(kmeans.inertia_)\n\nplt.plot(k_values, inertia, marker='o')\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Inertia\")\nplt.title(\"Elbow Method to Choose K\")\nplt.xticks(k_values)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that the optimal number of clusters is about 4.\n\n# choose k\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\necon_clusters = kmeans.fit_predict(X_pca)\n\nfor cluster in range(n_clusters):\n    plt.scatter(X_pca[econ_clusters == cluster, 0], X_pca[econ_clusters == cluster, 1],\n        label=f\"Cluster {cluster + 1}\", s=50, alpha=0.6)\n\nplt.xlabel(\"tSNE 1\")\nplt.ylabel(\"tSNE 2\")\nplt.title(\"K-means Clustering of Socioeconomic Factors\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ncluster_labels = kmeans.labels_\n\n\n\nsilhouette_avg = silhouette_score(X_scaled, cluster_labels)\nprint(f'Silhouette Score: {silhouette_avg}')\n\nSilhouette Score: 0.16605649350908025\n\n\nConclusion: there is not large separation between the clusters, indicating that we may need to use an alternate dimensionality reduction technique"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dbscan",
    "href": "technical-details/unsupervised-learning/main.html#dbscan",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "DBSCAN is an unsupervised learning clustering algorithm that is better suited for irregular data. It improves upon kmeans because it is able to find non linear clusters. It is also able to identify noise.\n\ndbscan = DBSCAN(eps=0.5, min_samples=3)\ndbscan.fit(subset)\ndf['DBSCAN_Cluster'] = dbscan.labels_\n\n\nnoise_points = df[df['DBSCAN_Cluster'] == -1]\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(subset[:, 0], subset[:, 1], c=dbscan.labels_, cmap='rainbow', s=20)\nplt.title(\"DBSCAN Clustering\")\nplt.xlabel(\"Feature 1 (Standardized)\")\nplt.ylabel(\"Feature 2 (Standardized)\")\nplt.colorbar(label=\"Cluster Label\")\nplt.show()\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nFile /opt/anaconda3/envs/dsan5400/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:173, in pandas._libs.index.IndexEngine.get_loc()\n\nTypeError: '(slice(None, None, None), 0)' is an invalid key\n\nDuring handling of the above exception, another exception occurred:\n\nInvalidIndexError                         Traceback (most recent call last)\nCell In[60], line 2\n      1 plt.figure(figsize=(10, 6))\n----&gt; 2 plt.scatter(subset[:, 0], subset[:, 1], c=dbscan.labels_, cmap='rainbow', s=20)\n      3 plt.title(\"DBSCAN Clustering\")\n      4 plt.xlabel(\"Feature 1 (Standardized)\")\n\nFile /opt/anaconda3/envs/dsan5400/lib/python3.11/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile /opt/anaconda3/envs/dsan5400/lib/python3.11/site-packages/pandas/core/indexes/base.py:3817, in Index.get_loc(self, key)\n   3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n-&gt; 3817     self._check_indexing_error(key)\n   3818     raise\n\nFile /opt/anaconda3/envs/dsan5400/lib/python3.11/site-packages/pandas/core/indexes/base.py:6059, in Index._check_indexing_error(self, key)\n   6055 def _check_indexing_error(self, key):\n   6056     if not is_scalar(key):\n   6057         # if key is not a scalar, directly raise an error (the code below\n   6058         # would convert to numpy arrays and raise later any way) - GH29926\n-&gt; 6059         raise InvalidIndexError(key)\n\nInvalidIndexError: (slice(None, None, None), 0)\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\nfrom sklearn.neighbors import NearestNeighbors\n\nneighbors = NearestNeighbors(n_neighbors=5)\nneighbors_fit = neighbors.fit(X_scaled)\ndistances, indices = neighbors_fit.kneighbors(X_scaled)\n\n# Sort distances for plotting\ndistances = np.sort(distances[:, 4], axis=0)\nplt.figure(figsize=(10, 6))\nplt.plot(distances)\nplt.title(\"k-Distance Plot\")\nplt.xlabel(\"Data Points Sorted\")\nplt.ylabel(\"4th Nearest Neighbor Distance\")\nplt.show()\n\n\n\n\n\n\n\n\n\neps_range = np.arange(0.1, 1.1, 0.1)\nmin_samples_values = range(2, 20)\n\nresults = []\n\nfor eps in eps_range:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(X_scaled)\n        if len(set(labels)) &gt; 1: \n            score = silhouette_score(X_scaled, labels)\n            results.append((eps, min_samples, score))\n\n\nresults_df = pd.DataFrame(results, columns=['eps', 'min_samples', 'silhouette_score'])\nbest_params = results_df.loc[results_df['silhouette_score'].idxmax()]\nbest_min_samples = best_params['min_samples']\nbest_eps = best_params['eps']\nprint(f\"Best eps: {best_eps}, Best min_samples: {best_min_samples}\")\n\nBest eps: 1.0, Best min_samples: 13.0\n\n\n\nclustering = DBSCAN(eps=best_eps, min_samples=int(best_min_samples))\nclustering_labels = clustering.fit_predict(X_scaled)\n\n\nplot = plt.scatter(df['bill_depth_mm'], df['flipper_length_mm'],c=clustering_labels)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title(\"DBSCAN Clustering\")\nunique_labels = np.unique(clustering_labels)\nlegend_labels = [\"Noise\" if label == -1 else f\"Cluster {label + 1}\" for label in unique_labels]\nhandles, _ = scatter.legend_elements()\nplt.legend(handles, legend_labels, title=\"Clusters\")\n\n\nfrom sklearn.decomposition import PCA\n\n# Apply PCA to reduce to 2D\npca = PCA(n_components=2)\ndf_pca = pca.fit_transform(df_courses)\n\n# Plot the 2D clustering\nplt.figure(figsize=(10, 6))\nplt.scatter(df_pca[:, 0], df_pca[:, 1], c=clustering_labels, cmap='viridis')\n\n# Adding labels and title\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.title(\"DBSCAN Clustering (PCA Reduced Dimensions)\")\n\n# Add legend for clusters\nunique_labels = np.unique(clustering_labels)\nlegend_labels = [\"Noise\" if label == -1 else f\"Cluster {label + 1}\" for label in unique_labels]\nhandles, _ = plt.gca().get_legend_handles_labels()\nplt.legend(handles, legend_labels, title=\"Clusters\")\n\n# Show plot\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\n\n# Example: Generate synthetic data\nX, y = make_blobs(n_samples=300, centers=4, random_state=42)\n\n# Perform Agglomerative Clustering\nagg_clust = AgglomerativeClustering(n_clusters=4, linkage='ward')\ny_pred = agg_clust.fit_predict(X_scaled)\n\n# Plot the result\nplt.figure(figsize=(8, 6))\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_pred, cmap='viridis')\nplt.title('Agglomerative Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n\n# Perform hierarchical/agglomerative clustering\nZ = linkage(subset, 'ward')\n\n# Create a dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(Z)\nplt.title('Dendrogram for Agglomerative Clustering')\nplt.xlabel('Data points')\nplt.ylabel('Distance')\nplt.show()"
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nMotivation: Explain the reasoning behind the work, such as solving a specific problem, improving a system, or optimizing performance.\nObjectives: Outline the specific outcomes you aim to achieve, whether it’s implementing a solution, analyzing data, or building a model.\n\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Jessica Joy is a full-time student in the Data Science and Analytics program at Georgetown University. She earned her Bachelor of Science in Financial Economics from Binghamton University in 2020. Before joining the DSAN program, she worked as a senior data analyst at EssenceMediacom, a marketing agency in New York City, where she contributed to various projects, including Marketing Mix Models and ad optimizations. She is excited to further develop her data skills and apply them to other industries, particularly social services."
  },
  {
    "objectID": "aboutme.html#education",
    "href": "aboutme.html#education",
    "title": "About Me",
    "section": "EDUCATION",
    "text": "EDUCATION\n\nExpected May 2026: Georgetown University: MS Data Science and Analytics Current relevant coursework: Data Science and Analytics, Probabilistic Modeling and Statistical Computing, Computational Linguistics with Advanced Python\nMay 2020: Binghamton University: BS Financial Economics Major GPA – 3.78/4.00 Involvement/Acitvities: Student-Athlete Success Center, Microeconomics Theory and Macroeconomics Tutor Alpha Epsilon Phi, Treasury Committee Helping Hand Project, Founding Member"
  },
  {
    "objectID": "aboutme.html#professional-experience",
    "href": "aboutme.html#professional-experience",
    "title": "About Me",
    "section": "PROFESSIONAL EXPERIENCE",
    "text": "PROFESSIONAL EXPERIENCE\n\nEssenceMediacom: New York, NY Senior Analyst, Data Science and Modeling June 2021 – August 2024 • Developed end-to-end Marketing Mix Models (MMM) for major clients, improving data processing and efficiency of 10M+ rows of creative data using Python, R, and SQL. Applied multiple linear regression modeling and validation to create final deliverables of actionable insights, resulting in increased marketing ROI. • Constructed a budget allocation tool using marketing mix model outputs, optimizing media mix strategies and driving up to a 30% increase in incremental revenue growth for clients. • Provided strategic consultation to Dell, leveraging vision AI data and advanced machine learning algorithms (Linear/Logistic Regression, K-Means Clustering, PCA) to analyze 200+ variables, potentially increasing KPIs by 130% through creative attribute optimization. • Strengthened new business pitches by building a database of 70+ marketing metrics for the “Remarkability Index”, effectively evaluating investments and securing new clients."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "High school dropout rates have a profound impact on students, families, and communities. To bring these numbers down, we first need to understand what’s driving them. This study takes a closer look at how the quality of schools and the socioeconomic challenges students face influence high school outcomes in New York City. By diving into these factors, we aim to uncover ways to help students succeed.\n\n\n\nHow does a school’s environment contribute to better student outcomes?\nWhat are the most significant predictors of student absenteeism, dropout rates and college persistence?\nWhat are the interactions between chronic absenteeism, dropout rates, and college persistence?\nHow does socioeconomic status impact student absenteeism, dropout rates and college persistence?\nHow can early academic indicators be used to identify schools at risk of high dropout or not persisting in college?\n\n\n\n\n\nCollect and clean data, creating a comprehensive dataset of NYC high schools\nPerform exploratory data analysis to uncover trends in the data\nPerform unsupervised learning and supervised learning to find real relationships between our features and targets: chronic absenteeism, dropout rates, and college persistence\nSummarize findings into actionable insights\n\n\n\n\nThis dataset contains 500 New York City public high schools. We can begin by taking a look at dropout rates across New York City Boroughs, revealing areas of needed intervention.\n\n\n\n\n\n\n\n\nIt is evident that the highest median drop out rate is in the Bronx, followed by Brooklyn. Although there are some high outliers in Queens. This highlights boroughs where intervention is necessary. To take a closer look, we can look at drop out rates by specific NYC school districts.\n\n\n\nThis plot clearly shows the highest drop out rate to be in district 12, which is located in the Bronx. Next is district 8 which is also located in the Bronx. This plot makes it easier to pinpoint school districts that need to be targeted.\nNext, we can take a look at specific socioeconomic and school features that we hypothesize will have an impact on our targets.\n\n\n\n\n\n\n\n\nUsing statistical testing, we were able to conclude that there is a statistically significant difference in mean median household income for high schools with high rates of drop out and high schools with low rates of dropout. This suggests that students from higher-income households, likely with parents with degrees and employment, and are less likely to drop out themselves. This emphasizes the importance of addressing economic disparities to improve these rates.\nSupportive Environment score comes from the NYC Quality report and is defined as, “reflects how well the school establishes a culture where students feel safe, challenged to grow, and supported to meet high expectations.” There is a statistically significant difference in the mean supportive environment score for a school with high and low rates of chronic absenteeism. This is important because it shows that when students feel good about the school environment, and feel a sense of value and community, theyre less likely to skip class or be absent.\n\n\n\n\n\n\n\n\n\n\n\nGrade 8 Proficiency is a crucial indicator of future success, highlighting the importance of early intervention to address academic challenges before they escalate\nSupportive and Trusting Environments contribute to better student outcomes, emphasizing the need for schools to create cultures where students feel safe, supported, and motivated\nEconomic Disadvantage plays a significant role in long-term outcomes such as college persistence, with students from lower-income backgrounds facing more barriers to succes\nThe Connection: Chronic absenteeism, dropout rates, and college persistence are closely linked. Addressing absenteeism early is key to preventing dropout and ensuring students stay on track for college success.\n\nBy identifying and understanding the key drivers, such as supportive environment and socioeconomic factors, we can develop targeted strategies to support students more effectively. Empowering schools to create safe, supportive, and inclusive environments, while addressing broader economic challenges, is essential to ensure that every student has the opportunity to succeed academically and beyond."
  },
  {
    "objectID": "report/report.html#breaking-the-cycle-key-findings-and-takeaways",
    "href": "report/report.html#breaking-the-cycle-key-findings-and-takeaways",
    "title": "Final Report",
    "section": "",
    "text": "High school dropout rates have a profound impact on students, families, and communities. To bring these numbers down, we first need to understand what’s driving them. This study takes a closer look at how the quality of schools and the socioeconomic challenges students face influence high school outcomes in New York City. By diving into these factors, we aim to uncover ways to help students succeed.\n\n\n\nHow does a school’s environment contribute to better student outcomes?\nWhat are the most significant predictors of student absenteeism, dropout rates and college persistence?\nWhat are the interactions between chronic absenteeism, dropout rates, and college persistence?\nHow does socioeconomic status impact student absenteeism, dropout rates and college persistence?\nHow can early academic indicators be used to identify schools at risk of high dropout or not persisting in college?\n\n\n\n\n\nCollect and clean data, creating a comprehensive dataset of NYC high schools\nPerform exploratory data analysis to uncover trends in the data\nPerform unsupervised learning and supervised learning to find real relationships between our features and targets: chronic absenteeism, dropout rates, and college persistence\nSummarize findings into actionable insights\n\n\n\n\nThis dataset contains 500 New York City public high schools. We can begin by taking a look at dropout rates across New York City Boroughs, revealing areas of needed intervention.\n\n\n\n\n\n\n\n\nIt is evident that the highest median drop out rate is in the Bronx, followed by Brooklyn. Although there are some high outliers in Queens. This highlights boroughs where intervention is necessary. To take a closer look, we can look at drop out rates by specific NYC school districts.\n\n\n\nThis plot clearly shows the highest drop out rate to be in district 12, which is located in the Bronx. Next is district 8 which is also located in the Bronx. This plot makes it easier to pinpoint school districts that need to be targeted.\nNext, we can take a look at specific socioeconomic and school features that we hypothesize will have an impact on our targets.\n\n\n\n\n\n\n\n\nUsing statistical testing, we were able to conclude that there is a statistically significant difference in mean median household income for high schools with high rates of drop out and high schools with low rates of dropout. This suggests that students from higher-income households, likely with parents with degrees and employment, and are less likely to drop out themselves. This emphasizes the importance of addressing economic disparities to improve these rates.\nSupportive Environment score comes from the NYC Quality report and is defined as, “reflects how well the school establishes a culture where students feel safe, challenged to grow, and supported to meet high expectations.” There is a statistically significant difference in the mean supportive environment score for a school with high and low rates of chronic absenteeism. This is important because it shows that when students feel good about the school environment, and feel a sense of value and community, theyre less likely to skip class or be absent.\n\n\n\n\n\n\n\n\n\n\n\nGrade 8 Proficiency is a crucial indicator of future success, highlighting the importance of early intervention to address academic challenges before they escalate\nSupportive and Trusting Environments contribute to better student outcomes, emphasizing the need for schools to create cultures where students feel safe, supported, and motivated\nEconomic Disadvantage plays a significant role in long-term outcomes such as college persistence, with students from lower-income backgrounds facing more barriers to succes\nThe Connection: Chronic absenteeism, dropout rates, and college persistence are closely linked. Addressing absenteeism early is key to preventing dropout and ensuring students stay on track for college success.\n\nBy identifying and understanding the key drivers, such as supportive environment and socioeconomic factors, we can develop targeted strategies to support students more effectively. Empowering schools to create safe, supportive, and inclusive environments, while addressing broader economic challenges, is essential to ensure that every student has the opportunity to succeed academically and beyond."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "link to “About Me” page\nProject contribution log:\nM: 11-04-2024\n\nbrainstorming topics with GPT\n\nT: 11-14-2024\n\nstarted literature review\nstarted collecting data\n\nM: 11-25-2024\n\nwrite up data cleaning section and insert code\n\nM: 11-26-2024\n\nwork on EDA and start supervised learning\n\nM: 12-8-2024\n\nadd random forest, boosting algorithms to regression\nwork on binary classification\n\nF: 12-12-2024 * [x] add Lasso regression\nF: 12-13-2024 * [x] explain machine learning methods"
  },
  {
    "objectID": "technical-details/progress-log.html#jessica-joy",
    "href": "technical-details/progress-log.html#jessica-joy",
    "title": "Progress log",
    "section": "",
    "text": "link to “About Me” page\nProject contribution log:\nM: 11-04-2024\n\nbrainstorming topics with GPT\n\nT: 11-14-2024\n\nstarted literature review\nstarted collecting data\n\nM: 11-25-2024\n\nwrite up data cleaning section and insert code\n\nM: 11-26-2024\n\nwork on EDA and start supervised learning\n\nM: 12-8-2024\n\nadd random forest, boosting algorithms to regression\nwork on binary classification\n\nF: 12-12-2024 * [x] add Lasso regression\nF: 12-13-2024 * [x] explain machine learning methods"
  }
]